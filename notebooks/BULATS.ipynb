{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BULATS dataset considerations\n",
    "\n",
    "The following notebook presentes the considerations on the BULATS dataset. It considers using old features as well as new ones and touches on topics such as the influence of the threshold set up on the F-score value and which features are the most important to the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pyAudioAnalysis import audioBasicIO\n",
    "from pyAudioAnalysis import audioFeatureExtraction\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.utils import class_weight\n",
    "from keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, Concatenate, Input, Embedding, Flatten\n",
    "from keras.models import Model, load_model\n",
    "import keras.backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we read in the csv files with the train and test features. We display them to show how the data essentially looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recording</th>\n",
       "      <th>segment</th>\n",
       "      <th>tokenNum</th>\n",
       "      <th>tokenStart</th>\n",
       "      <th>tokenEnd</th>\n",
       "      <th>token</th>\n",
       "      <th>boundary</th>\n",
       "      <th>prePause</th>\n",
       "      <th>postPause</th>\n",
       "      <th>phonDurFinal</th>\n",
       "      <th>...</th>\n",
       "      <th>EdiffEndWminE</th>\n",
       "      <th>EdiffMeanWminE</th>\n",
       "      <th>EdiffStartWminE</th>\n",
       "      <th>EdiffMeanWplus1minE</th>\n",
       "      <th>EmaxW</th>\n",
       "      <th>EminW</th>\n",
       "      <th>EmaxWplus1</th>\n",
       "      <th>EminWplus1</th>\n",
       "      <th>cefr</th>\n",
       "      <th>L1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S3X7KSJ2J2_SE_01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.890</td>\n",
       "      <td>1.995</td>\n",
       "      <td>i</td>\n",
       "      <td>0</td>\n",
       "      <td>1.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.105</td>\n",
       "      <td>...</td>\n",
       "      <td>3.612025</td>\n",
       "      <td>21.937102</td>\n",
       "      <td>77.662715</td>\n",
       "      <td>72.475252</td>\n",
       "      <td>20.154290</td>\n",
       "      <td>9.958136</td>\n",
       "      <td>73.725731</td>\n",
       "      <td>57.183466</td>\n",
       "      <td>C1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S3X7KSJ2J2_SE_01</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1.995</td>\n",
       "      <td>2.295</td>\n",
       "      <td>was</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>...</td>\n",
       "      <td>21.986764</td>\n",
       "      <td>18.690065</td>\n",
       "      <td>73.114418</td>\n",
       "      <td>70.384716</td>\n",
       "      <td>21.077025</td>\n",
       "      <td>21.937102</td>\n",
       "      <td>77.662715</td>\n",
       "      <td>72.475252</td>\n",
       "      <td>C1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S3X7KSJ2J2_SE_01</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2.295</td>\n",
       "      <td>2.395</td>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>...</td>\n",
       "      <td>18.815168</td>\n",
       "      <td>14.976877</td>\n",
       "      <td>74.316966</td>\n",
       "      <td>60.495767</td>\n",
       "      <td>16.813275</td>\n",
       "      <td>18.690065</td>\n",
       "      <td>73.114418</td>\n",
       "      <td>70.384716</td>\n",
       "      <td>C1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S3X7KSJ2J2_SE_01</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2.395</td>\n",
       "      <td>2.895</td>\n",
       "      <td>retail</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>...</td>\n",
       "      <td>15.689467</td>\n",
       "      <td>3.441999</td>\n",
       "      <td>60.538246</td>\n",
       "      <td>54.232943</td>\n",
       "      <td>8.292119</td>\n",
       "      <td>14.976877</td>\n",
       "      <td>74.316966</td>\n",
       "      <td>60.495767</td>\n",
       "      <td>C1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S3X7KSJ2J2_SE_01</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2.895</td>\n",
       "      <td>3.195</td>\n",
       "      <td>shop</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>...</td>\n",
       "      <td>3.350403</td>\n",
       "      <td>5.139980</td>\n",
       "      <td>60.635598</td>\n",
       "      <td>56.599095</td>\n",
       "      <td>6.966805</td>\n",
       "      <td>3.441999</td>\n",
       "      <td>60.538246</td>\n",
       "      <td>54.232943</td>\n",
       "      <td>C1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          recording  segment  tokenNum  tokenStart  tokenEnd   token  \\\n",
       "0  S3X7KSJ2J2_SE_01        1         2       1.890     1.995       i   \n",
       "1  S3X7KSJ2J2_SE_01        1         3       1.995     2.295     was   \n",
       "2  S3X7KSJ2J2_SE_01        1         4       2.295     2.395       a   \n",
       "3  S3X7KSJ2J2_SE_01        1         5       2.395     2.895  retail   \n",
       "4  S3X7KSJ2J2_SE_01        1         6       2.895     3.195    shop   \n",
       "\n",
       "   boundary  prePause  postPause  phonDurFinal ...  EdiffEndWminE  \\\n",
       "0         0      1.89        0.0         0.105 ...       3.612025   \n",
       "1         0      0.00        0.0         0.100 ...      21.986764   \n",
       "2         0      0.00        0.0         0.100 ...      18.815168   \n",
       "3         0      0.00        0.0         0.100 ...      15.689467   \n",
       "4         0      0.00        0.0         0.100 ...       3.350403   \n",
       "\n",
       "   EdiffMeanWminE  EdiffStartWminE  EdiffMeanWplus1minE      EmaxW      EminW  \\\n",
       "0       21.937102        77.662715            72.475252  20.154290   9.958136   \n",
       "1       18.690065        73.114418            70.384716  21.077025  21.937102   \n",
       "2       14.976877        74.316966            60.495767  16.813275  18.690065   \n",
       "3        3.441999        60.538246            54.232943   8.292119  14.976877   \n",
       "4        5.139980        60.635598            56.599095   6.966805   3.441999   \n",
       "\n",
       "   EmaxWplus1  EminWplus1  cefr  L1  \n",
       "0   73.725731   57.183466    C1 NaN  \n",
       "1   77.662715   72.475252    C1 NaN  \n",
       "2   73.114418   70.384716    C1 NaN  \n",
       "3   74.316966   60.495767    C1 NaN  \n",
       "4   60.538246   54.232943    C1 NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('data/BULATS/train.csv')\n",
    "test = pd.read_csv('data/BULATS/test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by exploring some new features. As we don't want to use the old features we need to come up with a new set of them. There are multiple options that have been considered in this research. Here we present a routine that implementes a reasonably well known library called puAudioAnalysis (https://github.com/tyiannak/pyAudioAnalysis). It extracts multiple features such as the zero crossing rate, energy, entropy of it, MFCC's, etc (more in the github wiki). We once again operate on time steps instead of pure time. The library allows us to define how big the window we are considering is and how long the time step that we jump by is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, window, time_step):\n",
    "    features = []\n",
    "    pause = []\n",
    "    y = []\n",
    "    \n",
    "    unique_recordings = df['recording'].unique()\n",
    "    for file_index, recording in enumerate(unique_recordings):\n",
    "        name = f'/usr/groups/alta/speech/BULATS/BLXXXeval1/audio/wav1dir/{recording}.wav'\n",
    "        \n",
    "        Fs, x = audioBasicIO.readAudioFile(name)\n",
    "        audio_features, _ = audioFeatureExtraction.stFeatureExtraction(x, Fs, window*Fs, time_step*Fs)\n",
    "        audio_features = audio_features.transpose()\n",
    "        \n",
    "        for index, row in df.loc[df['recording'] == recording].iterrows():\n",
    "            start = int(row['tokenStart']/time_step)\n",
    "            finish = int(row['tokenEnd']/time_step)\n",
    "\n",
    "            features.append(audio_features[start:finish + 1])\n",
    "            pause.append(row['postPause'])\n",
    "            y.append(row['boundary'])\n",
    "            \n",
    "        print(f\"Done {file_index + 1} out of {len(unique_recordings)}, {(file_index + 1)/len(unique_recordings)}\")\n",
    "        \n",
    "    return [features, pause], y        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we prepare the data using the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1 out of 1353, 0.0007390983000739098\n",
      "Done 2 out of 1353, 0.0014781966001478197\n",
      "Done 3 out of 1353, 0.0022172949002217295\n",
      "Done 4 out of 1353, 0.0029563932002956393\n",
      "Done 5 out of 1353, 0.003695491500369549\n",
      "Done 6 out of 1353, 0.004434589800443459\n",
      "Done 7 out of 1353, 0.005173688100517369\n",
      "Done 8 out of 1353, 0.005912786400591279\n",
      "Done 9 out of 1353, 0.0066518847006651885\n",
      "Done 10 out of 1353, 0.007390983000739098\n",
      "Done 11 out of 1353, 0.008130081300813009\n",
      "Done 12 out of 1353, 0.008869179600886918\n",
      "Done 13 out of 1353, 0.009608277900960829\n",
      "Done 14 out of 1353, 0.010347376201034738\n",
      "Done 15 out of 1353, 0.011086474501108648\n",
      "Done 16 out of 1353, 0.011825572801182557\n",
      "Done 17 out of 1353, 0.012564671101256468\n",
      "Done 18 out of 1353, 0.013303769401330377\n",
      "Done 19 out of 1353, 0.014042867701404288\n",
      "Done 20 out of 1353, 0.014781966001478197\n",
      "Done 21 out of 1353, 0.015521064301552107\n",
      "Done 22 out of 1353, 0.016260162601626018\n",
      "Done 23 out of 1353, 0.016999260901699925\n",
      "Done 24 out of 1353, 0.017738359201773836\n",
      "Done 25 out of 1353, 0.018477457501847747\n",
      "Done 26 out of 1353, 0.019216555801921657\n",
      "Done 27 out of 1353, 0.019955654101995565\n",
      "Done 28 out of 1353, 0.020694752402069475\n",
      "Done 29 out of 1353, 0.021433850702143386\n",
      "Done 30 out of 1353, 0.022172949002217297\n",
      "Done 31 out of 1353, 0.022912047302291204\n",
      "Done 32 out of 1353, 0.023651145602365115\n",
      "Done 33 out of 1353, 0.024390243902439025\n",
      "Done 34 out of 1353, 0.025129342202512936\n",
      "Done 35 out of 1353, 0.025868440502586843\n",
      "Done 36 out of 1353, 0.026607538802660754\n",
      "Done 37 out of 1353, 0.027346637102734665\n",
      "Done 38 out of 1353, 0.028085735402808575\n",
      "Done 39 out of 1353, 0.028824833702882482\n",
      "Done 40 out of 1353, 0.029563932002956393\n",
      "Done 41 out of 1353, 0.030303030303030304\n",
      "Done 42 out of 1353, 0.031042128603104215\n",
      "Done 43 out of 1353, 0.031781226903178125\n",
      "Done 44 out of 1353, 0.032520325203252036\n",
      "Done 45 out of 1353, 0.03325942350332594\n",
      "Done 46 out of 1353, 0.03399852180339985\n",
      "Done 47 out of 1353, 0.03473762010347376\n",
      "Done 48 out of 1353, 0.03547671840354767\n",
      "Done 49 out of 1353, 0.03621581670362158\n",
      "Done 50 out of 1353, 0.03695491500369549\n",
      "Done 51 out of 1353, 0.037694013303769404\n",
      "Done 52 out of 1353, 0.038433111603843315\n",
      "Done 53 out of 1353, 0.03917220990391722\n",
      "Done 54 out of 1353, 0.03991130820399113\n",
      "Done 55 out of 1353, 0.04065040650406504\n",
      "Done 56 out of 1353, 0.04138950480413895\n",
      "Done 57 out of 1353, 0.04212860310421286\n",
      "Done 58 out of 1353, 0.04286770140428677\n",
      "Done 59 out of 1353, 0.04360679970436068\n",
      "Done 60 out of 1353, 0.04434589800443459\n",
      "Done 61 out of 1353, 0.0450849963045085\n",
      "Done 62 out of 1353, 0.04582409460458241\n",
      "Done 63 out of 1353, 0.04656319290465632\n",
      "Done 64 out of 1353, 0.04730229120473023\n",
      "Done 65 out of 1353, 0.04804138950480414\n",
      "Done 66 out of 1353, 0.04878048780487805\n",
      "Done 67 out of 1353, 0.04951958610495196\n",
      "Done 68 out of 1353, 0.05025868440502587\n",
      "Done 69 out of 1353, 0.050997782705099776\n",
      "Done 70 out of 1353, 0.051736881005173686\n",
      "Done 71 out of 1353, 0.0524759793052476\n",
      "Done 72 out of 1353, 0.05321507760532151\n",
      "Done 73 out of 1353, 0.05395417590539542\n",
      "Done 74 out of 1353, 0.05469327420546933\n",
      "Done 75 out of 1353, 0.05543237250554324\n",
      "Done 76 out of 1353, 0.05617147080561715\n",
      "Done 77 out of 1353, 0.056910569105691054\n",
      "Done 78 out of 1353, 0.057649667405764965\n",
      "Done 79 out of 1353, 0.058388765705838876\n",
      "Done 80 out of 1353, 0.059127864005912786\n",
      "Done 81 out of 1353, 0.0598669623059867\n",
      "Done 82 out of 1353, 0.06060606060606061\n",
      "Done 83 out of 1353, 0.06134515890613452\n",
      "Done 84 out of 1353, 0.06208425720620843\n",
      "Done 85 out of 1353, 0.06282335550628233\n",
      "Done 86 out of 1353, 0.06356245380635625\n",
      "Done 87 out of 1353, 0.06430155210643015\n",
      "Done 88 out of 1353, 0.06504065040650407\n",
      "Done 89 out of 1353, 0.06577974870657798\n",
      "Done 90 out of 1353, 0.06651884700665188\n",
      "Done 91 out of 1353, 0.0672579453067258\n",
      "Done 92 out of 1353, 0.0679970436067997\n",
      "Done 93 out of 1353, 0.06873614190687362\n",
      "Done 94 out of 1353, 0.06947524020694752\n",
      "Done 95 out of 1353, 0.07021433850702144\n",
      "Done 96 out of 1353, 0.07095343680709534\n",
      "Done 97 out of 1353, 0.07169253510716925\n",
      "Done 98 out of 1353, 0.07243163340724317\n",
      "Done 99 out of 1353, 0.07317073170731707\n",
      "Done 100 out of 1353, 0.07390983000739099\n",
      "Done 101 out of 1353, 0.07464892830746489\n",
      "Done 102 out of 1353, 0.07538802660753881\n",
      "Done 103 out of 1353, 0.07612712490761271\n",
      "Done 104 out of 1353, 0.07686622320768663\n",
      "Done 105 out of 1353, 0.07760532150776053\n",
      "Done 106 out of 1353, 0.07834441980783444\n",
      "Done 107 out of 1353, 0.07908351810790835\n",
      "Done 108 out of 1353, 0.07982261640798226\n",
      "Done 109 out of 1353, 0.08056171470805618\n",
      "Done 110 out of 1353, 0.08130081300813008\n",
      "Done 111 out of 1353, 0.082039911308204\n",
      "Done 112 out of 1353, 0.0827790096082779\n",
      "Done 113 out of 1353, 0.0835181079083518\n",
      "Done 114 out of 1353, 0.08425720620842572\n",
      "Done 115 out of 1353, 0.08499630450849963\n",
      "Done 116 out of 1353, 0.08573540280857354\n",
      "Done 117 out of 1353, 0.08647450110864745\n",
      "Done 118 out of 1353, 0.08721359940872137\n",
      "Done 119 out of 1353, 0.08795269770879527\n",
      "Done 120 out of 1353, 0.08869179600886919\n",
      "Done 121 out of 1353, 0.08943089430894309\n",
      "Done 122 out of 1353, 0.090169992609017\n",
      "Done 123 out of 1353, 0.09090909090909091\n",
      "Done 124 out of 1353, 0.09164818920916482\n",
      "Done 125 out of 1353, 0.09238728750923873\n",
      "Done 126 out of 1353, 0.09312638580931264\n",
      "Done 127 out of 1353, 0.09386548410938655\n",
      "Done 128 out of 1353, 0.09460458240946046\n",
      "Done 129 out of 1353, 0.09534368070953436\n",
      "Done 130 out of 1353, 0.09608277900960828\n",
      "Done 131 out of 1353, 0.09682187730968218\n",
      "Done 132 out of 1353, 0.0975609756097561\n",
      "Done 133 out of 1353, 0.09830007390983\n",
      "Done 134 out of 1353, 0.09903917220990392\n",
      "Done 135 out of 1353, 0.09977827050997783\n",
      "Done 136 out of 1353, 0.10051736881005174\n",
      "Done 137 out of 1353, 0.10125646711012565\n",
      "Done 138 out of 1353, 0.10199556541019955\n",
      "Done 139 out of 1353, 0.10273466371027347\n",
      "Done 140 out of 1353, 0.10347376201034737\n",
      "Done 141 out of 1353, 0.10421286031042129\n",
      "Done 142 out of 1353, 0.1049519586104952\n",
      "Done 143 out of 1353, 0.10569105691056911\n",
      "Done 144 out of 1353, 0.10643015521064302\n",
      "Done 145 out of 1353, 0.10716925351071692\n",
      "Done 146 out of 1353, 0.10790835181079084\n",
      "Done 147 out of 1353, 0.10864745011086474\n",
      "Done 148 out of 1353, 0.10938654841093866\n",
      "Done 149 out of 1353, 0.11012564671101256\n",
      "Done 150 out of 1353, 0.11086474501108648\n",
      "Done 151 out of 1353, 0.11160384331116038\n",
      "Done 152 out of 1353, 0.1123429416112343\n",
      "Done 153 out of 1353, 0.1130820399113082\n",
      "Done 154 out of 1353, 0.11382113821138211\n",
      "Done 155 out of 1353, 0.11456023651145603\n",
      "Done 156 out of 1353, 0.11529933481152993\n",
      "Done 157 out of 1353, 0.11603843311160385\n",
      "Done 158 out of 1353, 0.11677753141167775\n",
      "Done 159 out of 1353, 0.11751662971175167\n",
      "Done 160 out of 1353, 0.11825572801182557\n",
      "Done 161 out of 1353, 0.11899482631189948\n",
      "Done 162 out of 1353, 0.1197339246119734\n",
      "Done 163 out of 1353, 0.1204730229120473\n",
      "Done 164 out of 1353, 0.12121212121212122\n",
      "Done 165 out of 1353, 0.12195121951219512\n",
      "Done 166 out of 1353, 0.12269031781226904\n",
      "Done 167 out of 1353, 0.12342941611234294\n",
      "Done 168 out of 1353, 0.12416851441241686\n",
      "Done 169 out of 1353, 0.12490761271249076\n",
      "Done 170 out of 1353, 0.12564671101256467\n",
      "Done 171 out of 1353, 0.12638580931263857\n",
      "Done 172 out of 1353, 0.1271249076127125\n",
      "Done 173 out of 1353, 0.1278640059127864\n",
      "Done 174 out of 1353, 0.1286031042128603\n",
      "Done 175 out of 1353, 0.1293422025129342\n",
      "Done 176 out of 1353, 0.13008130081300814\n",
      "Done 177 out of 1353, 0.13082039911308205\n",
      "Done 178 out of 1353, 0.13155949741315595\n",
      "Done 179 out of 1353, 0.13229859571322986\n",
      "Done 180 out of 1353, 0.13303769401330376\n",
      "Done 181 out of 1353, 0.1337767923133777\n",
      "Done 182 out of 1353, 0.1345158906134516\n",
      "Done 183 out of 1353, 0.1352549889135255\n",
      "Done 184 out of 1353, 0.1359940872135994\n",
      "Done 185 out of 1353, 0.1367331855136733\n",
      "Done 186 out of 1353, 0.13747228381374724\n",
      "Done 187 out of 1353, 0.13821138211382114\n",
      "Done 188 out of 1353, 0.13895048041389504\n",
      "Done 189 out of 1353, 0.13968957871396895\n",
      "Done 190 out of 1353, 0.14042867701404288\n",
      "Done 191 out of 1353, 0.14116777531411678\n",
      "Done 192 out of 1353, 0.1419068736141907\n",
      "Done 193 out of 1353, 0.1426459719142646\n",
      "Done 194 out of 1353, 0.1433850702143385\n",
      "Done 195 out of 1353, 0.14412416851441243\n",
      "Done 196 out of 1353, 0.14486326681448633\n",
      "Done 197 out of 1353, 0.14560236511456023\n",
      "Done 198 out of 1353, 0.14634146341463414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 199 out of 1353, 0.14708056171470807\n",
      "Done 200 out of 1353, 0.14781966001478197\n",
      "Done 201 out of 1353, 0.14855875831485588\n",
      "Done 202 out of 1353, 0.14929785661492978\n",
      "Done 203 out of 1353, 0.15003695491500368\n",
      "Done 204 out of 1353, 0.15077605321507762\n",
      "Done 205 out of 1353, 0.15151515151515152\n",
      "Done 206 out of 1353, 0.15225424981522542\n",
      "Done 207 out of 1353, 0.15299334811529933\n",
      "Done 208 out of 1353, 0.15373244641537326\n",
      "Done 209 out of 1353, 0.15447154471544716\n",
      "Done 210 out of 1353, 0.15521064301552107\n",
      "Done 211 out of 1353, 0.15594974131559497\n",
      "Done 212 out of 1353, 0.15668883961566887\n",
      "Done 213 out of 1353, 0.1574279379157428\n",
      "Done 214 out of 1353, 0.1581670362158167\n",
      "Done 215 out of 1353, 0.1589061345158906\n",
      "Done 216 out of 1353, 0.15964523281596452\n",
      "Done 217 out of 1353, 0.16038433111603842\n",
      "Done 218 out of 1353, 0.16112342941611235\n",
      "Done 219 out of 1353, 0.16186252771618626\n",
      "Done 220 out of 1353, 0.16260162601626016\n",
      "Done 221 out of 1353, 0.16334072431633406\n",
      "Done 222 out of 1353, 0.164079822616408\n",
      "Done 223 out of 1353, 0.1648189209164819\n",
      "Done 224 out of 1353, 0.1655580192165558\n",
      "Done 225 out of 1353, 0.1662971175166297\n",
      "Done 226 out of 1353, 0.1670362158167036\n",
      "Done 227 out of 1353, 0.16777531411677754\n",
      "Done 228 out of 1353, 0.16851441241685144\n",
      "Done 229 out of 1353, 0.16925351071692535\n",
      "Done 230 out of 1353, 0.16999260901699925\n",
      "Done 231 out of 1353, 0.17073170731707318\n",
      "Done 232 out of 1353, 0.1714708056171471\n",
      "Done 233 out of 1353, 0.172209903917221\n",
      "Done 234 out of 1353, 0.1729490022172949\n",
      "Done 235 out of 1353, 0.1736881005173688\n",
      "Done 236 out of 1353, 0.17442719881744273\n",
      "Done 237 out of 1353, 0.17516629711751663\n",
      "Done 238 out of 1353, 0.17590539541759054\n",
      "Done 239 out of 1353, 0.17664449371766444\n",
      "Done 240 out of 1353, 0.17738359201773837\n",
      "Done 241 out of 1353, 0.17812269031781228\n",
      "Done 242 out of 1353, 0.17886178861788618\n",
      "Done 243 out of 1353, 0.17960088691796008\n",
      "Done 244 out of 1353, 0.180339985218034\n",
      "Done 245 out of 1353, 0.18107908351810792\n",
      "Done 246 out of 1353, 0.18181818181818182\n",
      "Done 247 out of 1353, 0.18255728011825573\n",
      "Done 248 out of 1353, 0.18329637841832963\n",
      "Done 249 out of 1353, 0.18403547671840353\n",
      "Done 250 out of 1353, 0.18477457501847747\n",
      "Done 251 out of 1353, 0.18551367331855137\n",
      "Done 252 out of 1353, 0.18625277161862527\n",
      "Done 253 out of 1353, 0.18699186991869918\n",
      "Done 254 out of 1353, 0.1877309682187731\n",
      "Done 255 out of 1353, 0.188470066518847\n",
      "Done 256 out of 1353, 0.18920916481892092\n",
      "Done 257 out of 1353, 0.18994826311899482\n",
      "Done 258 out of 1353, 0.19068736141906872\n",
      "Done 259 out of 1353, 0.19142645971914266\n",
      "Done 260 out of 1353, 0.19216555801921656\n",
      "Done 261 out of 1353, 0.19290465631929046\n",
      "Done 262 out of 1353, 0.19364375461936437\n",
      "Done 263 out of 1353, 0.1943828529194383\n",
      "Done 264 out of 1353, 0.1951219512195122\n",
      "Done 265 out of 1353, 0.1958610495195861\n",
      "Done 266 out of 1353, 0.19660014781966\n",
      "Done 267 out of 1353, 0.1973392461197339\n",
      "Done 268 out of 1353, 0.19807834441980784\n",
      "Done 269 out of 1353, 0.19881744271988175\n",
      "Done 270 out of 1353, 0.19955654101995565\n",
      "Done 271 out of 1353, 0.20029563932002956\n",
      "Done 272 out of 1353, 0.2010347376201035\n",
      "Done 273 out of 1353, 0.2017738359201774\n",
      "Done 274 out of 1353, 0.2025129342202513\n",
      "Done 275 out of 1353, 0.2032520325203252\n",
      "Done 276 out of 1353, 0.2039911308203991\n",
      "Done 277 out of 1353, 0.20473022912047303\n",
      "Done 278 out of 1353, 0.20546932742054694\n",
      "Done 279 out of 1353, 0.20620842572062084\n",
      "Done 280 out of 1353, 0.20694752402069475\n",
      "Done 281 out of 1353, 0.20768662232076865\n",
      "Done 282 out of 1353, 0.20842572062084258\n",
      "Done 283 out of 1353, 0.20916481892091648\n",
      "Done 284 out of 1353, 0.2099039172209904\n",
      "Done 285 out of 1353, 0.2106430155210643\n",
      "Done 286 out of 1353, 0.21138211382113822\n",
      "Done 287 out of 1353, 0.21212121212121213\n",
      "Done 288 out of 1353, 0.21286031042128603\n",
      "Done 289 out of 1353, 0.21359940872135993\n",
      "Done 290 out of 1353, 0.21433850702143384\n",
      "Done 291 out of 1353, 0.21507760532150777\n",
      "Done 292 out of 1353, 0.21581670362158167\n",
      "Done 293 out of 1353, 0.21655580192165558\n",
      "Done 294 out of 1353, 0.21729490022172948\n",
      "Done 295 out of 1353, 0.2180339985218034\n",
      "Done 296 out of 1353, 0.21877309682187732\n",
      "Done 297 out of 1353, 0.21951219512195122\n",
      "Done 298 out of 1353, 0.22025129342202512\n",
      "Done 299 out of 1353, 0.22099039172209903\n",
      "Done 300 out of 1353, 0.22172949002217296\n",
      "Done 301 out of 1353, 0.22246858832224686\n",
      "Done 302 out of 1353, 0.22320768662232077\n",
      "Done 303 out of 1353, 0.22394678492239467\n",
      "Done 304 out of 1353, 0.2246858832224686\n",
      "Done 305 out of 1353, 0.2254249815225425\n",
      "Done 306 out of 1353, 0.2261640798226164\n",
      "Done 307 out of 1353, 0.2269031781226903\n",
      "Done 308 out of 1353, 0.22764227642276422\n",
      "Done 309 out of 1353, 0.22838137472283815\n",
      "Done 310 out of 1353, 0.22912047302291205\n",
      "Done 311 out of 1353, 0.22985957132298596\n",
      "Done 312 out of 1353, 0.23059866962305986\n",
      "Done 313 out of 1353, 0.23133776792313376\n",
      "Done 314 out of 1353, 0.2320768662232077\n",
      "Done 315 out of 1353, 0.2328159645232816\n",
      "Done 316 out of 1353, 0.2335550628233555\n",
      "Done 317 out of 1353, 0.2342941611234294\n",
      "Done 318 out of 1353, 0.23503325942350334\n",
      "Done 319 out of 1353, 0.23577235772357724\n",
      "Done 320 out of 1353, 0.23651145602365115\n",
      "Done 321 out of 1353, 0.23725055432372505\n",
      "Done 322 out of 1353, 0.23798965262379895\n",
      "Done 323 out of 1353, 0.23872875092387288\n",
      "Done 324 out of 1353, 0.2394678492239468\n",
      "Done 325 out of 1353, 0.2402069475240207\n",
      "Done 326 out of 1353, 0.2409460458240946\n",
      "Done 327 out of 1353, 0.24168514412416853\n",
      "Done 328 out of 1353, 0.24242424242424243\n",
      "Done 329 out of 1353, 0.24316334072431633\n",
      "Done 330 out of 1353, 0.24390243902439024\n",
      "Done 331 out of 1353, 0.24464153732446414\n",
      "Done 332 out of 1353, 0.24538063562453807\n",
      "Done 333 out of 1353, 0.24611973392461198\n",
      "Done 334 out of 1353, 0.24685883222468588\n",
      "Done 335 out of 1353, 0.24759793052475979\n",
      "Done 336 out of 1353, 0.24833702882483372\n",
      "Done 337 out of 1353, 0.24907612712490762\n",
      "Done 338 out of 1353, 0.24981522542498152\n",
      "Done 339 out of 1353, 0.25055432372505543\n",
      "Done 340 out of 1353, 0.25129342202512933\n",
      "Done 341 out of 1353, 0.25203252032520324\n",
      "Done 342 out of 1353, 0.25277161862527714\n",
      "Done 343 out of 1353, 0.2535107169253511\n",
      "Done 344 out of 1353, 0.254249815225425\n",
      "Done 345 out of 1353, 0.2549889135254989\n",
      "Done 346 out of 1353, 0.2557280118255728\n",
      "Done 347 out of 1353, 0.2564671101256467\n",
      "Done 348 out of 1353, 0.2572062084257206\n",
      "Done 349 out of 1353, 0.2579453067257945\n",
      "Done 350 out of 1353, 0.2586844050258684\n",
      "Done 351 out of 1353, 0.25942350332594233\n",
      "Done 352 out of 1353, 0.2601626016260163\n",
      "Done 353 out of 1353, 0.2609016999260902\n",
      "Done 354 out of 1353, 0.2616407982261641\n",
      "Done 355 out of 1353, 0.262379896526238\n",
      "Done 356 out of 1353, 0.2631189948263119\n",
      "Done 357 out of 1353, 0.2638580931263858\n",
      "Done 358 out of 1353, 0.2645971914264597\n",
      "Done 359 out of 1353, 0.2653362897265336\n",
      "Done 360 out of 1353, 0.2660753880266075\n",
      "Done 361 out of 1353, 0.2668144863266814\n",
      "Done 362 out of 1353, 0.2675535846267554\n",
      "Done 363 out of 1353, 0.2682926829268293\n",
      "Done 364 out of 1353, 0.2690317812269032\n",
      "Done 365 out of 1353, 0.2697708795269771\n",
      "Done 366 out of 1353, 0.270509977827051\n",
      "Done 367 out of 1353, 0.2712490761271249\n",
      "Done 368 out of 1353, 0.2719881744271988\n",
      "Done 369 out of 1353, 0.2727272727272727\n",
      "Done 370 out of 1353, 0.2734663710273466\n",
      "Done 371 out of 1353, 0.27420546932742057\n",
      "Done 372 out of 1353, 0.2749445676274945\n",
      "Done 373 out of 1353, 0.2756836659275684\n",
      "Done 374 out of 1353, 0.2764227642276423\n",
      "Done 375 out of 1353, 0.2771618625277162\n",
      "Done 376 out of 1353, 0.2779009608277901\n",
      "Done 377 out of 1353, 0.278640059127864\n",
      "Done 378 out of 1353, 0.2793791574279379\n",
      "Done 379 out of 1353, 0.2801182557280118\n",
      "Done 380 out of 1353, 0.28085735402808576\n",
      "Done 381 out of 1353, 0.28159645232815966\n",
      "Done 382 out of 1353, 0.28233555062823357\n",
      "Done 383 out of 1353, 0.28307464892830747\n",
      "Done 384 out of 1353, 0.2838137472283814\n",
      "Done 385 out of 1353, 0.2845528455284553\n",
      "Done 386 out of 1353, 0.2852919438285292\n",
      "Done 387 out of 1353, 0.2860310421286031\n",
      "Done 388 out of 1353, 0.286770140428677\n",
      "Done 389 out of 1353, 0.28750923872875095\n",
      "Done 390 out of 1353, 0.28824833702882485\n",
      "Done 391 out of 1353, 0.28898743532889876\n",
      "Done 392 out of 1353, 0.28972653362897266\n",
      "Done 393 out of 1353, 0.29046563192904656\n",
      "Done 394 out of 1353, 0.29120473022912047\n",
      "Done 395 out of 1353, 0.29194382852919437\n",
      "Done 396 out of 1353, 0.2926829268292683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 397 out of 1353, 0.2934220251293422\n",
      "Done 398 out of 1353, 0.29416112342941614\n",
      "Done 399 out of 1353, 0.29490022172949004\n",
      "Done 400 out of 1353, 0.29563932002956395\n",
      "Done 401 out of 1353, 0.29637841832963785\n",
      "Done 402 out of 1353, 0.29711751662971175\n",
      "Done 403 out of 1353, 0.29785661492978566\n",
      "Done 404 out of 1353, 0.29859571322985956\n",
      "Done 405 out of 1353, 0.29933481152993346\n",
      "Done 406 out of 1353, 0.30007390983000737\n",
      "Done 407 out of 1353, 0.3008130081300813\n",
      "Done 408 out of 1353, 0.30155210643015523\n",
      "Done 409 out of 1353, 0.30229120473022913\n",
      "Done 410 out of 1353, 0.30303030303030304\n",
      "Done 411 out of 1353, 0.30376940133037694\n",
      "Done 412 out of 1353, 0.30450849963045085\n",
      "Done 413 out of 1353, 0.30524759793052475\n",
      "Done 414 out of 1353, 0.30598669623059865\n",
      "Done 415 out of 1353, 0.30672579453067256\n",
      "Done 416 out of 1353, 0.3074648928307465\n",
      "Done 417 out of 1353, 0.3082039911308204\n",
      "Done 418 out of 1353, 0.3089430894308943\n",
      "Done 419 out of 1353, 0.30968218773096823\n",
      "Done 420 out of 1353, 0.31042128603104213\n",
      "Done 421 out of 1353, 0.31116038433111604\n",
      "Done 422 out of 1353, 0.31189948263118994\n",
      "Done 423 out of 1353, 0.31263858093126384\n",
      "Done 424 out of 1353, 0.31337767923133775\n",
      "Done 425 out of 1353, 0.31411677753141165\n",
      "Done 426 out of 1353, 0.3148558758314856\n",
      "Done 427 out of 1353, 0.3155949741315595\n",
      "Done 428 out of 1353, 0.3163340724316334\n",
      "Done 429 out of 1353, 0.3170731707317073\n",
      "Done 430 out of 1353, 0.3178122690317812\n",
      "Done 431 out of 1353, 0.31855136733185513\n",
      "Done 432 out of 1353, 0.31929046563192903\n",
      "Done 433 out of 1353, 0.32002956393200294\n",
      "Done 434 out of 1353, 0.32076866223207684\n",
      "Done 435 out of 1353, 0.3215077605321508\n",
      "Done 436 out of 1353, 0.3222468588322247\n",
      "Done 437 out of 1353, 0.3229859571322986\n",
      "Done 438 out of 1353, 0.3237250554323725\n",
      "Done 439 out of 1353, 0.3244641537324464\n",
      "Done 440 out of 1353, 0.3252032520325203\n",
      "Done 441 out of 1353, 0.3259423503325942\n",
      "Done 442 out of 1353, 0.3266814486326681\n",
      "Done 443 out of 1353, 0.32742054693274203\n",
      "Done 444 out of 1353, 0.328159645232816\n",
      "Done 445 out of 1353, 0.3288987435328899\n",
      "Done 446 out of 1353, 0.3296378418329638\n",
      "Done 447 out of 1353, 0.3303769401330377\n",
      "Done 448 out of 1353, 0.3311160384331116\n",
      "Done 449 out of 1353, 0.3318551367331855\n",
      "Done 450 out of 1353, 0.3325942350332594\n",
      "Done 451 out of 1353, 0.3333333333333333\n",
      "Done 452 out of 1353, 0.3340724316334072\n",
      "Done 453 out of 1353, 0.3348115299334812\n",
      "Done 454 out of 1353, 0.3355506282335551\n",
      "Done 455 out of 1353, 0.336289726533629\n",
      "Done 456 out of 1353, 0.3370288248337029\n",
      "Done 457 out of 1353, 0.3377679231337768\n",
      "Done 458 out of 1353, 0.3385070214338507\n",
      "Done 459 out of 1353, 0.3392461197339246\n",
      "Done 460 out of 1353, 0.3399852180339985\n",
      "Done 461 out of 1353, 0.3407243163340724\n",
      "Done 462 out of 1353, 0.34146341463414637\n",
      "Done 463 out of 1353, 0.34220251293422027\n",
      "Done 464 out of 1353, 0.3429416112342942\n",
      "Done 465 out of 1353, 0.3436807095343681\n",
      "Done 466 out of 1353, 0.344419807834442\n",
      "Done 467 out of 1353, 0.3451589061345159\n",
      "Done 468 out of 1353, 0.3458980044345898\n",
      "Done 469 out of 1353, 0.3466371027346637\n",
      "Done 470 out of 1353, 0.3473762010347376\n",
      "Done 471 out of 1353, 0.34811529933481156\n",
      "Done 472 out of 1353, 0.34885439763488546\n",
      "Done 473 out of 1353, 0.34959349593495936\n",
      "Done 474 out of 1353, 0.35033259423503327\n",
      "Done 475 out of 1353, 0.35107169253510717\n",
      "Done 476 out of 1353, 0.3518107908351811\n",
      "Done 477 out of 1353, 0.352549889135255\n",
      "Done 478 out of 1353, 0.3532889874353289\n",
      "Done 479 out of 1353, 0.3540280857354028\n",
      "Done 480 out of 1353, 0.35476718403547675\n",
      "Done 481 out of 1353, 0.35550628233555065\n",
      "Done 482 out of 1353, 0.35624538063562455\n",
      "Done 483 out of 1353, 0.35698447893569846\n",
      "Done 484 out of 1353, 0.35772357723577236\n",
      "Done 485 out of 1353, 0.35846267553584626\n",
      "Done 486 out of 1353, 0.35920177383592017\n",
      "Done 487 out of 1353, 0.35994087213599407\n",
      "Done 488 out of 1353, 0.360679970436068\n",
      "Done 489 out of 1353, 0.3614190687361419\n",
      "Done 490 out of 1353, 0.36215816703621584\n",
      "Done 491 out of 1353, 0.36289726533628974\n",
      "Done 492 out of 1353, 0.36363636363636365\n",
      "Done 493 out of 1353, 0.36437546193643755\n",
      "Done 494 out of 1353, 0.36511456023651145\n",
      "Done 495 out of 1353, 0.36585365853658536\n",
      "Done 496 out of 1353, 0.36659275683665926\n",
      "Done 497 out of 1353, 0.36733185513673317\n",
      "Done 498 out of 1353, 0.36807095343680707\n",
      "Done 499 out of 1353, 0.36881005173688103\n",
      "Done 500 out of 1353, 0.36954915003695493\n",
      "Done 501 out of 1353, 0.37028824833702884\n",
      "Done 502 out of 1353, 0.37102734663710274\n",
      "Done 503 out of 1353, 0.37176644493717664\n",
      "Done 504 out of 1353, 0.37250554323725055\n",
      "Done 505 out of 1353, 0.37324464153732445\n",
      "Done 506 out of 1353, 0.37398373983739835\n",
      "Done 507 out of 1353, 0.37472283813747226\n",
      "Done 508 out of 1353, 0.3754619364375462\n",
      "Done 509 out of 1353, 0.3762010347376201\n",
      "Done 510 out of 1353, 0.376940133037694\n",
      "Done 511 out of 1353, 0.37767923133776793\n",
      "Done 512 out of 1353, 0.37841832963784183\n",
      "Done 513 out of 1353, 0.37915742793791574\n",
      "Done 514 out of 1353, 0.37989652623798964\n",
      "Done 515 out of 1353, 0.38063562453806354\n",
      "Done 516 out of 1353, 0.38137472283813745\n",
      "Done 517 out of 1353, 0.3821138211382114\n",
      "Done 518 out of 1353, 0.3828529194382853\n",
      "Done 519 out of 1353, 0.3835920177383592\n",
      "Done 520 out of 1353, 0.3843311160384331\n",
      "Done 521 out of 1353, 0.385070214338507\n",
      "Done 522 out of 1353, 0.3858093126385809\n",
      "Done 523 out of 1353, 0.38654841093865483\n",
      "Done 524 out of 1353, 0.38728750923872873\n",
      "Done 525 out of 1353, 0.38802660753880264\n",
      "Done 526 out of 1353, 0.3887657058388766\n",
      "Done 527 out of 1353, 0.3895048041389505\n",
      "Done 528 out of 1353, 0.3902439024390244\n",
      "Done 529 out of 1353, 0.3909830007390983\n",
      "Done 530 out of 1353, 0.3917220990391722\n",
      "Done 531 out of 1353, 0.3924611973392461\n",
      "Done 532 out of 1353, 0.39320029563932\n",
      "Done 533 out of 1353, 0.3939393939393939\n",
      "Done 534 out of 1353, 0.3946784922394678\n",
      "Done 535 out of 1353, 0.3954175905395418\n",
      "Done 536 out of 1353, 0.3961566888396157\n",
      "Done 537 out of 1353, 0.3968957871396896\n",
      "Done 538 out of 1353, 0.3976348854397635\n",
      "Done 539 out of 1353, 0.3983739837398374\n",
      "Done 540 out of 1353, 0.3991130820399113\n",
      "Done 541 out of 1353, 0.3998521803399852\n",
      "Done 542 out of 1353, 0.4005912786400591\n",
      "Done 543 out of 1353, 0.401330376940133\n",
      "Done 544 out of 1353, 0.402069475240207\n",
      "Done 545 out of 1353, 0.4028085735402809\n",
      "Done 546 out of 1353, 0.4035476718403548\n",
      "Done 547 out of 1353, 0.4042867701404287\n",
      "Done 548 out of 1353, 0.4050258684405026\n",
      "Done 549 out of 1353, 0.4057649667405765\n",
      "Done 550 out of 1353, 0.4065040650406504\n",
      "Done 551 out of 1353, 0.4072431633407243\n",
      "Done 552 out of 1353, 0.4079822616407982\n",
      "Done 553 out of 1353, 0.4087213599408721\n",
      "Done 554 out of 1353, 0.40946045824094607\n",
      "Done 555 out of 1353, 0.41019955654101997\n",
      "Done 556 out of 1353, 0.4109386548410939\n",
      "Done 557 out of 1353, 0.4116777531411678\n",
      "Done 558 out of 1353, 0.4124168514412417\n",
      "Done 559 out of 1353, 0.4131559497413156\n",
      "Done 560 out of 1353, 0.4138950480413895\n",
      "Done 561 out of 1353, 0.4146341463414634\n",
      "Done 562 out of 1353, 0.4153732446415373\n",
      "Done 563 out of 1353, 0.41611234294161126\n",
      "Done 564 out of 1353, 0.41685144124168516\n",
      "Done 565 out of 1353, 0.41759053954175906\n",
      "Done 566 out of 1353, 0.41832963784183297\n",
      "Done 567 out of 1353, 0.4190687361419069\n",
      "Done 568 out of 1353, 0.4198078344419808\n",
      "Done 569 out of 1353, 0.4205469327420547\n",
      "Done 570 out of 1353, 0.4212860310421286\n",
      "Done 571 out of 1353, 0.4220251293422025\n",
      "Done 572 out of 1353, 0.42276422764227645\n",
      "Done 573 out of 1353, 0.42350332594235035\n",
      "Done 574 out of 1353, 0.42424242424242425\n",
      "Done 575 out of 1353, 0.42498152254249816\n",
      "Done 576 out of 1353, 0.42572062084257206\n",
      "Done 577 out of 1353, 0.42645971914264597\n",
      "Done 578 out of 1353, 0.42719881744271987\n",
      "Done 579 out of 1353, 0.4279379157427938\n",
      "Done 580 out of 1353, 0.4286770140428677\n",
      "Done 581 out of 1353, 0.42941611234294164\n",
      "Done 582 out of 1353, 0.43015521064301554\n",
      "Done 583 out of 1353, 0.43089430894308944\n",
      "Done 584 out of 1353, 0.43163340724316335\n",
      "Done 585 out of 1353, 0.43237250554323725\n",
      "Done 586 out of 1353, 0.43311160384331115\n",
      "Done 587 out of 1353, 0.43385070214338506\n",
      "Done 588 out of 1353, 0.43458980044345896\n",
      "Done 589 out of 1353, 0.43532889874353287\n",
      "Done 590 out of 1353, 0.4360679970436068\n",
      "Done 591 out of 1353, 0.43680709534368073\n",
      "Done 592 out of 1353, 0.43754619364375463\n",
      "Done 593 out of 1353, 0.43828529194382854\n",
      "Done 594 out of 1353, 0.43902439024390244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 595 out of 1353, 0.43976348854397634\n",
      "Done 596 out of 1353, 0.44050258684405025\n",
      "Done 597 out of 1353, 0.44124168514412415\n",
      "Done 598 out of 1353, 0.44198078344419806\n",
      "Done 599 out of 1353, 0.442719881744272\n",
      "Done 600 out of 1353, 0.4434589800443459\n",
      "Done 601 out of 1353, 0.4441980783444198\n",
      "Done 602 out of 1353, 0.4449371766444937\n",
      "Done 603 out of 1353, 0.44567627494456763\n",
      "Done 604 out of 1353, 0.44641537324464153\n",
      "Done 605 out of 1353, 0.44715447154471544\n",
      "Done 606 out of 1353, 0.44789356984478934\n",
      "Done 607 out of 1353, 0.44863266814486324\n",
      "Done 608 out of 1353, 0.4493717664449372\n",
      "Done 609 out of 1353, 0.4501108647450111\n",
      "Done 610 out of 1353, 0.450849963045085\n",
      "Done 611 out of 1353, 0.4515890613451589\n",
      "Done 612 out of 1353, 0.4523281596452328\n",
      "Done 613 out of 1353, 0.4530672579453067\n",
      "Done 614 out of 1353, 0.4538063562453806\n",
      "Done 615 out of 1353, 0.45454545454545453\n",
      "Done 616 out of 1353, 0.45528455284552843\n",
      "Done 617 out of 1353, 0.45602365114560234\n",
      "Done 618 out of 1353, 0.4567627494456763\n",
      "Done 619 out of 1353, 0.4575018477457502\n",
      "Done 620 out of 1353, 0.4582409460458241\n",
      "Done 621 out of 1353, 0.458980044345898\n",
      "Done 622 out of 1353, 0.4597191426459719\n",
      "Done 623 out of 1353, 0.4604582409460458\n",
      "Done 624 out of 1353, 0.4611973392461197\n",
      "Done 625 out of 1353, 0.4619364375461936\n",
      "Done 626 out of 1353, 0.4626755358462675\n",
      "Done 627 out of 1353, 0.4634146341463415\n",
      "Done 628 out of 1353, 0.4641537324464154\n",
      "Done 629 out of 1353, 0.4648928307464893\n",
      "Done 630 out of 1353, 0.4656319290465632\n",
      "Done 631 out of 1353, 0.4663710273466371\n",
      "Done 632 out of 1353, 0.467110125646711\n",
      "Done 633 out of 1353, 0.4678492239467849\n",
      "Done 634 out of 1353, 0.4685883222468588\n",
      "Done 635 out of 1353, 0.4693274205469327\n",
      "Done 636 out of 1353, 0.4700665188470067\n",
      "Done 637 out of 1353, 0.4708056171470806\n",
      "Done 638 out of 1353, 0.4715447154471545\n",
      "Done 639 out of 1353, 0.4722838137472284\n",
      "Done 640 out of 1353, 0.4730229120473023\n",
      "Done 641 out of 1353, 0.4737620103473762\n",
      "Done 642 out of 1353, 0.4745011086474501\n",
      "Done 643 out of 1353, 0.475240206947524\n",
      "Done 644 out of 1353, 0.4759793052475979\n",
      "Done 645 out of 1353, 0.47671840354767187\n",
      "Done 646 out of 1353, 0.47745750184774577\n",
      "Done 647 out of 1353, 0.4781966001478197\n",
      "Done 648 out of 1353, 0.4789356984478936\n",
      "Done 649 out of 1353, 0.4796747967479675\n",
      "Done 650 out of 1353, 0.4804138950480414\n",
      "Done 651 out of 1353, 0.4811529933481153\n",
      "Done 652 out of 1353, 0.4818920916481892\n",
      "Done 653 out of 1353, 0.4826311899482631\n",
      "Done 654 out of 1353, 0.48337028824833705\n",
      "Done 655 out of 1353, 0.48410938654841096\n",
      "Done 656 out of 1353, 0.48484848484848486\n",
      "Done 657 out of 1353, 0.48558758314855877\n",
      "Done 658 out of 1353, 0.48632668144863267\n",
      "Done 659 out of 1353, 0.4870657797487066\n",
      "Done 660 out of 1353, 0.4878048780487805\n",
      "Done 661 out of 1353, 0.4885439763488544\n",
      "Done 662 out of 1353, 0.4892830746489283\n",
      "Done 663 out of 1353, 0.49002217294900224\n",
      "Done 664 out of 1353, 0.49076127124907615\n",
      "Done 665 out of 1353, 0.49150036954915005\n",
      "Done 666 out of 1353, 0.49223946784922396\n",
      "Done 667 out of 1353, 0.49297856614929786\n",
      "Done 668 out of 1353, 0.49371766444937176\n",
      "Done 669 out of 1353, 0.49445676274944567\n",
      "Done 670 out of 1353, 0.49519586104951957\n",
      "Done 671 out of 1353, 0.4959349593495935\n",
      "Done 672 out of 1353, 0.49667405764966743\n",
      "Done 673 out of 1353, 0.49741315594974134\n",
      "Done 674 out of 1353, 0.49815225424981524\n",
      "Done 675 out of 1353, 0.49889135254988914\n",
      "Done 676 out of 1353, 0.49963045084996305\n",
      "Done 677 out of 1353, 0.500369549150037\n",
      "Done 678 out of 1353, 0.5011086474501109\n",
      "Done 679 out of 1353, 0.5018477457501848\n",
      "Done 680 out of 1353, 0.5025868440502587\n",
      "Done 681 out of 1353, 0.5033259423503326\n",
      "Done 682 out of 1353, 0.5040650406504065\n",
      "Done 683 out of 1353, 0.5048041389504804\n",
      "Done 684 out of 1353, 0.5055432372505543\n",
      "Done 685 out of 1353, 0.5062823355506282\n",
      "Done 686 out of 1353, 0.5070214338507022\n",
      "Done 687 out of 1353, 0.5077605321507761\n",
      "Done 688 out of 1353, 0.50849963045085\n",
      "Done 689 out of 1353, 0.5092387287509239\n",
      "Done 690 out of 1353, 0.5099778270509978\n",
      "Done 691 out of 1353, 0.5107169253510717\n",
      "Done 692 out of 1353, 0.5114560236511456\n",
      "Done 693 out of 1353, 0.5121951219512195\n",
      "Done 694 out of 1353, 0.5129342202512934\n",
      "Done 695 out of 1353, 0.5136733185513673\n",
      "Done 696 out of 1353, 0.5144124168514412\n",
      "Done 697 out of 1353, 0.5151515151515151\n",
      "Done 698 out of 1353, 0.515890613451589\n",
      "Done 699 out of 1353, 0.516629711751663\n",
      "Done 700 out of 1353, 0.5173688100517368\n",
      "Done 701 out of 1353, 0.5181079083518108\n",
      "Done 702 out of 1353, 0.5188470066518847\n",
      "Done 703 out of 1353, 0.5195861049519586\n",
      "Done 704 out of 1353, 0.5203252032520326\n",
      "Done 705 out of 1353, 0.5210643015521065\n",
      "Done 706 out of 1353, 0.5218033998521804\n",
      "Done 707 out of 1353, 0.5225424981522543\n",
      "Done 708 out of 1353, 0.5232815964523282\n",
      "Done 709 out of 1353, 0.5240206947524021\n",
      "Done 710 out of 1353, 0.524759793052476\n",
      "Done 711 out of 1353, 0.5254988913525499\n",
      "Done 712 out of 1353, 0.5262379896526238\n",
      "Done 713 out of 1353, 0.5269770879526977\n",
      "Done 714 out of 1353, 0.5277161862527716\n",
      "Done 715 out of 1353, 0.5284552845528455\n",
      "Done 716 out of 1353, 0.5291943828529194\n",
      "Done 717 out of 1353, 0.5299334811529933\n",
      "Done 718 out of 1353, 0.5306725794530672\n",
      "Done 719 out of 1353, 0.5314116777531411\n",
      "Done 720 out of 1353, 0.532150776053215\n",
      "Done 721 out of 1353, 0.5328898743532889\n",
      "Done 722 out of 1353, 0.5336289726533628\n",
      "Done 723 out of 1353, 0.5343680709534369\n",
      "Done 724 out of 1353, 0.5351071692535108\n",
      "Done 725 out of 1353, 0.5358462675535847\n",
      "Done 726 out of 1353, 0.5365853658536586\n",
      "Done 727 out of 1353, 0.5373244641537325\n",
      "Done 728 out of 1353, 0.5380635624538064\n",
      "Done 729 out of 1353, 0.5388026607538803\n",
      "Done 730 out of 1353, 0.5395417590539542\n",
      "Done 731 out of 1353, 0.5402808573540281\n",
      "Done 732 out of 1353, 0.541019955654102\n",
      "Done 733 out of 1353, 0.5417590539541759\n",
      "Done 734 out of 1353, 0.5424981522542498\n",
      "Done 735 out of 1353, 0.5432372505543237\n",
      "Done 736 out of 1353, 0.5439763488543976\n",
      "Done 737 out of 1353, 0.5447154471544715\n",
      "Done 738 out of 1353, 0.5454545454545454\n",
      "Done 739 out of 1353, 0.5461936437546193\n",
      "Done 740 out of 1353, 0.5469327420546932\n",
      "Done 741 out of 1353, 0.5476718403547672\n",
      "Done 742 out of 1353, 0.5484109386548411\n",
      "Done 743 out of 1353, 0.549150036954915\n",
      "Done 744 out of 1353, 0.549889135254989\n",
      "Done 745 out of 1353, 0.5506282335550629\n",
      "Done 746 out of 1353, 0.5513673318551368\n",
      "Done 747 out of 1353, 0.5521064301552107\n",
      "Done 748 out of 1353, 0.5528455284552846\n",
      "Done 749 out of 1353, 0.5535846267553585\n",
      "Done 750 out of 1353, 0.5543237250554324\n",
      "Done 751 out of 1353, 0.5550628233555063\n",
      "Done 752 out of 1353, 0.5558019216555802\n",
      "Done 753 out of 1353, 0.5565410199556541\n",
      "Done 754 out of 1353, 0.557280118255728\n",
      "Done 755 out of 1353, 0.5580192165558019\n",
      "Done 756 out of 1353, 0.5587583148558758\n",
      "Done 757 out of 1353, 0.5594974131559497\n",
      "Done 758 out of 1353, 0.5602365114560236\n",
      "Done 759 out of 1353, 0.5609756097560976\n",
      "Done 760 out of 1353, 0.5617147080561715\n",
      "Done 761 out of 1353, 0.5624538063562454\n",
      "Done 762 out of 1353, 0.5631929046563193\n",
      "Done 763 out of 1353, 0.5639320029563932\n",
      "Done 764 out of 1353, 0.5646711012564671\n",
      "Done 765 out of 1353, 0.565410199556541\n",
      "Done 766 out of 1353, 0.5661492978566149\n",
      "Done 767 out of 1353, 0.5668883961566888\n",
      "Done 768 out of 1353, 0.5676274944567627\n",
      "Done 769 out of 1353, 0.5683665927568367\n",
      "Done 770 out of 1353, 0.5691056910569106\n",
      "Done 771 out of 1353, 0.5698447893569845\n",
      "Done 772 out of 1353, 0.5705838876570584\n",
      "Done 773 out of 1353, 0.5713229859571323\n",
      "Done 774 out of 1353, 0.5720620842572062\n",
      "Done 775 out of 1353, 0.5728011825572801\n",
      "Done 776 out of 1353, 0.573540280857354\n",
      "Done 777 out of 1353, 0.5742793791574279\n",
      "Done 778 out of 1353, 0.5750184774575019\n",
      "Done 779 out of 1353, 0.5757575757575758\n",
      "Done 780 out of 1353, 0.5764966740576497\n",
      "Done 781 out of 1353, 0.5772357723577236\n",
      "Done 782 out of 1353, 0.5779748706577975\n",
      "Done 783 out of 1353, 0.5787139689578714\n",
      "Done 784 out of 1353, 0.5794530672579453\n",
      "Done 785 out of 1353, 0.5801921655580192\n",
      "Done 786 out of 1353, 0.5809312638580931\n",
      "Done 787 out of 1353, 0.581670362158167\n",
      "Done 788 out of 1353, 0.5824094604582409\n",
      "Done 789 out of 1353, 0.5831485587583148\n",
      "Done 790 out of 1353, 0.5838876570583887\n",
      "Done 791 out of 1353, 0.5846267553584626\n",
      "Done 792 out of 1353, 0.5853658536585366\n",
      "Done 793 out of 1353, 0.5861049519586105\n",
      "Done 794 out of 1353, 0.5868440502586844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 795 out of 1353, 0.5875831485587583\n",
      "Done 796 out of 1353, 0.5883222468588323\n",
      "Done 797 out of 1353, 0.5890613451589062\n",
      "Done 798 out of 1353, 0.5898004434589801\n",
      "Done 799 out of 1353, 0.590539541759054\n",
      "Done 800 out of 1353, 0.5912786400591279\n",
      "Done 801 out of 1353, 0.5920177383592018\n",
      "Done 802 out of 1353, 0.5927568366592757\n",
      "Done 803 out of 1353, 0.5934959349593496\n",
      "Done 804 out of 1353, 0.5942350332594235\n",
      "Done 805 out of 1353, 0.5949741315594974\n",
      "Done 806 out of 1353, 0.5957132298595713\n",
      "Done 807 out of 1353, 0.5964523281596452\n",
      "Done 808 out of 1353, 0.5971914264597191\n",
      "Done 809 out of 1353, 0.597930524759793\n",
      "Done 810 out of 1353, 0.5986696230598669\n",
      "Done 811 out of 1353, 0.5994087213599408\n",
      "Done 812 out of 1353, 0.6001478196600147\n",
      "Done 813 out of 1353, 0.6008869179600886\n",
      "Done 814 out of 1353, 0.6016260162601627\n",
      "Done 815 out of 1353, 0.6023651145602366\n",
      "Done 816 out of 1353, 0.6031042128603105\n",
      "Done 817 out of 1353, 0.6038433111603844\n",
      "Done 818 out of 1353, 0.6045824094604583\n",
      "Done 819 out of 1353, 0.6053215077605322\n",
      "Done 820 out of 1353, 0.6060606060606061\n",
      "Done 821 out of 1353, 0.60679970436068\n",
      "Done 822 out of 1353, 0.6075388026607539\n",
      "Done 823 out of 1353, 0.6082779009608278\n",
      "Done 824 out of 1353, 0.6090169992609017\n",
      "Done 825 out of 1353, 0.6097560975609756\n",
      "Done 826 out of 1353, 0.6104951958610495\n",
      "Done 827 out of 1353, 0.6112342941611234\n",
      "Done 828 out of 1353, 0.6119733924611973\n",
      "Done 829 out of 1353, 0.6127124907612712\n",
      "Done 830 out of 1353, 0.6134515890613451\n",
      "Done 831 out of 1353, 0.614190687361419\n",
      "Done 832 out of 1353, 0.614929785661493\n",
      "Done 833 out of 1353, 0.6156688839615669\n",
      "Done 834 out of 1353, 0.6164079822616408\n",
      "Done 835 out of 1353, 0.6171470805617147\n",
      "Done 836 out of 1353, 0.6178861788617886\n",
      "Done 837 out of 1353, 0.6186252771618626\n",
      "Done 838 out of 1353, 0.6193643754619365\n",
      "Done 839 out of 1353, 0.6201034737620104\n",
      "Done 840 out of 1353, 0.6208425720620843\n",
      "Done 841 out of 1353, 0.6215816703621582\n",
      "Done 842 out of 1353, 0.6223207686622321\n",
      "Done 843 out of 1353, 0.623059866962306\n",
      "Done 844 out of 1353, 0.6237989652623799\n",
      "Done 845 out of 1353, 0.6245380635624538\n",
      "Done 846 out of 1353, 0.6252771618625277\n",
      "Done 847 out of 1353, 0.6260162601626016\n",
      "Done 848 out of 1353, 0.6267553584626755\n",
      "Done 849 out of 1353, 0.6274944567627494\n",
      "Done 850 out of 1353, 0.6282335550628233\n",
      "Done 851 out of 1353, 0.6289726533628973\n",
      "Done 852 out of 1353, 0.6297117516629712\n",
      "Done 853 out of 1353, 0.6304508499630451\n",
      "Done 854 out of 1353, 0.631189948263119\n",
      "Done 855 out of 1353, 0.6319290465631929\n",
      "Done 856 out of 1353, 0.6326681448632668\n",
      "Done 857 out of 1353, 0.6334072431633407\n",
      "Done 858 out of 1353, 0.6341463414634146\n",
      "Done 859 out of 1353, 0.6348854397634885\n",
      "Done 860 out of 1353, 0.6356245380635624\n",
      "Done 861 out of 1353, 0.6363636363636364\n",
      "Done 862 out of 1353, 0.6371027346637103\n",
      "Done 863 out of 1353, 0.6378418329637842\n",
      "Done 864 out of 1353, 0.6385809312638581\n",
      "Done 865 out of 1353, 0.639320029563932\n",
      "Done 866 out of 1353, 0.6400591278640059\n",
      "Done 867 out of 1353, 0.6407982261640798\n",
      "Done 868 out of 1353, 0.6415373244641537\n",
      "Done 869 out of 1353, 0.6422764227642277\n",
      "Done 870 out of 1353, 0.6430155210643016\n",
      "Done 871 out of 1353, 0.6437546193643755\n",
      "Done 872 out of 1353, 0.6444937176644494\n",
      "Done 873 out of 1353, 0.6452328159645233\n",
      "Done 874 out of 1353, 0.6459719142645972\n",
      "Done 875 out of 1353, 0.6467110125646711\n",
      "Done 876 out of 1353, 0.647450110864745\n",
      "Done 877 out of 1353, 0.6481892091648189\n",
      "Done 878 out of 1353, 0.6489283074648928\n",
      "Done 879 out of 1353, 0.6496674057649667\n",
      "Done 880 out of 1353, 0.6504065040650406\n",
      "Done 881 out of 1353, 0.6511456023651145\n",
      "Done 882 out of 1353, 0.6518847006651884\n",
      "Done 883 out of 1353, 0.6526237989652623\n",
      "Done 884 out of 1353, 0.6533628972653363\n",
      "Done 885 out of 1353, 0.6541019955654102\n",
      "Done 886 out of 1353, 0.6548410938654841\n",
      "Done 887 out of 1353, 0.6555801921655581\n",
      "Done 888 out of 1353, 0.656319290465632\n",
      "Done 889 out of 1353, 0.6570583887657059\n",
      "Done 890 out of 1353, 0.6577974870657798\n",
      "Done 891 out of 1353, 0.6585365853658537\n",
      "Done 892 out of 1353, 0.6592756836659276\n",
      "Done 893 out of 1353, 0.6600147819660015\n",
      "Done 894 out of 1353, 0.6607538802660754\n",
      "Done 895 out of 1353, 0.6614929785661493\n",
      "Done 896 out of 1353, 0.6622320768662232\n",
      "Done 897 out of 1353, 0.6629711751662971\n",
      "Done 898 out of 1353, 0.663710273466371\n",
      "Done 899 out of 1353, 0.6644493717664449\n",
      "Done 900 out of 1353, 0.6651884700665188\n",
      "Done 901 out of 1353, 0.6659275683665927\n",
      "Done 902 out of 1353, 0.6666666666666666\n",
      "Done 903 out of 1353, 0.6674057649667405\n",
      "Done 904 out of 1353, 0.6681448632668144\n",
      "Done 905 out of 1353, 0.6688839615668883\n",
      "Done 906 out of 1353, 0.6696230598669624\n",
      "Done 907 out of 1353, 0.6703621581670363\n",
      "Done 908 out of 1353, 0.6711012564671102\n",
      "Done 909 out of 1353, 0.6718403547671841\n",
      "Done 910 out of 1353, 0.672579453067258\n",
      "Done 911 out of 1353, 0.6733185513673319\n",
      "Done 912 out of 1353, 0.6740576496674058\n",
      "Done 913 out of 1353, 0.6747967479674797\n",
      "Done 914 out of 1353, 0.6755358462675536\n",
      "Done 915 out of 1353, 0.6762749445676275\n",
      "Done 916 out of 1353, 0.6770140428677014\n",
      "Done 917 out of 1353, 0.6777531411677753\n",
      "Done 918 out of 1353, 0.6784922394678492\n",
      "Done 919 out of 1353, 0.6792313377679231\n",
      "Done 920 out of 1353, 0.679970436067997\n",
      "Done 921 out of 1353, 0.6807095343680709\n",
      "Done 922 out of 1353, 0.6814486326681448\n",
      "Done 923 out of 1353, 0.6821877309682187\n",
      "Done 924 out of 1353, 0.6829268292682927\n",
      "Done 925 out of 1353, 0.6836659275683666\n",
      "Done 926 out of 1353, 0.6844050258684405\n",
      "Done 927 out of 1353, 0.6851441241685144\n",
      "Done 928 out of 1353, 0.6858832224685883\n",
      "Done 929 out of 1353, 0.6866223207686623\n",
      "Done 930 out of 1353, 0.6873614190687362\n",
      "Done 931 out of 1353, 0.6881005173688101\n",
      "Done 932 out of 1353, 0.688839615668884\n",
      "Done 933 out of 1353, 0.6895787139689579\n",
      "Done 934 out of 1353, 0.6903178122690318\n",
      "Done 935 out of 1353, 0.6910569105691057\n",
      "Done 936 out of 1353, 0.6917960088691796\n",
      "Done 937 out of 1353, 0.6925351071692535\n",
      "Done 938 out of 1353, 0.6932742054693274\n",
      "Done 939 out of 1353, 0.6940133037694013\n",
      "Done 940 out of 1353, 0.6947524020694752\n",
      "Done 941 out of 1353, 0.6954915003695491\n",
      "Done 942 out of 1353, 0.6962305986696231\n",
      "Done 943 out of 1353, 0.696969696969697\n",
      "Done 944 out of 1353, 0.6977087952697709\n",
      "Done 945 out of 1353, 0.6984478935698448\n",
      "Done 946 out of 1353, 0.6991869918699187\n",
      "Done 947 out of 1353, 0.6999260901699926\n",
      "Done 948 out of 1353, 0.7006651884700665\n",
      "Done 949 out of 1353, 0.7014042867701404\n",
      "Done 950 out of 1353, 0.7021433850702143\n",
      "Done 951 out of 1353, 0.7028824833702882\n",
      "Done 952 out of 1353, 0.7036215816703622\n",
      "Done 953 out of 1353, 0.704360679970436\n",
      "Done 954 out of 1353, 0.70509977827051\n",
      "Done 955 out of 1353, 0.7058388765705839\n",
      "Done 956 out of 1353, 0.7065779748706578\n",
      "Done 957 out of 1353, 0.7073170731707317\n",
      "Done 958 out of 1353, 0.7080561714708056\n",
      "Done 959 out of 1353, 0.7087952697708795\n",
      "Done 960 out of 1353, 0.7095343680709535\n",
      "Done 961 out of 1353, 0.7102734663710274\n",
      "Done 962 out of 1353, 0.7110125646711013\n",
      "Done 963 out of 1353, 0.7117516629711752\n",
      "Done 964 out of 1353, 0.7124907612712491\n",
      "Done 965 out of 1353, 0.713229859571323\n",
      "Done 966 out of 1353, 0.7139689578713969\n",
      "Done 967 out of 1353, 0.7147080561714708\n",
      "Done 968 out of 1353, 0.7154471544715447\n",
      "Done 969 out of 1353, 0.7161862527716186\n",
      "Done 970 out of 1353, 0.7169253510716925\n",
      "Done 971 out of 1353, 0.7176644493717664\n",
      "Done 972 out of 1353, 0.7184035476718403\n",
      "Done 973 out of 1353, 0.7191426459719142\n",
      "Done 974 out of 1353, 0.7198817442719881\n",
      "Done 975 out of 1353, 0.720620842572062\n",
      "Done 976 out of 1353, 0.721359940872136\n",
      "Done 977 out of 1353, 0.7220990391722099\n",
      "Done 978 out of 1353, 0.7228381374722838\n",
      "Done 979 out of 1353, 0.7235772357723578\n",
      "Done 980 out of 1353, 0.7243163340724317\n",
      "Done 981 out of 1353, 0.7250554323725056\n",
      "Done 982 out of 1353, 0.7257945306725795\n",
      "Done 983 out of 1353, 0.7265336289726534\n",
      "Done 984 out of 1353, 0.7272727272727273\n",
      "Done 985 out of 1353, 0.7280118255728012\n",
      "Done 986 out of 1353, 0.7287509238728751\n",
      "Done 987 out of 1353, 0.729490022172949\n",
      "Done 988 out of 1353, 0.7302291204730229\n",
      "Done 989 out of 1353, 0.7309682187730968\n",
      "Done 990 out of 1353, 0.7317073170731707\n",
      "Done 991 out of 1353, 0.7324464153732446\n",
      "Done 992 out of 1353, 0.7331855136733185\n",
      "Done 993 out of 1353, 0.7339246119733924\n",
      "Done 994 out of 1353, 0.7346637102734663\n",
      "Done 995 out of 1353, 0.7354028085735402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 996 out of 1353, 0.7361419068736141\n",
      "Done 997 out of 1353, 0.7368810051736882\n",
      "Done 998 out of 1353, 0.7376201034737621\n",
      "Done 999 out of 1353, 0.738359201773836\n",
      "Done 1000 out of 1353, 0.7390983000739099\n",
      "Done 1001 out of 1353, 0.7398373983739838\n",
      "Done 1002 out of 1353, 0.7405764966740577\n",
      "Done 1003 out of 1353, 0.7413155949741316\n",
      "Done 1004 out of 1353, 0.7420546932742055\n",
      "Done 1005 out of 1353, 0.7427937915742794\n",
      "Done 1006 out of 1353, 0.7435328898743533\n",
      "Done 1007 out of 1353, 0.7442719881744272\n",
      "Done 1008 out of 1353, 0.7450110864745011\n",
      "Done 1009 out of 1353, 0.745750184774575\n",
      "Done 1010 out of 1353, 0.7464892830746489\n",
      "Done 1011 out of 1353, 0.7472283813747228\n",
      "Done 1012 out of 1353, 0.7479674796747967\n",
      "Done 1013 out of 1353, 0.7487065779748706\n",
      "Done 1014 out of 1353, 0.7494456762749445\n",
      "Done 1015 out of 1353, 0.7501847745750185\n",
      "Done 1016 out of 1353, 0.7509238728750924\n",
      "Done 1017 out of 1353, 0.7516629711751663\n",
      "Done 1018 out of 1353, 0.7524020694752402\n",
      "Done 1019 out of 1353, 0.7531411677753141\n",
      "Done 1020 out of 1353, 0.753880266075388\n",
      "Done 1021 out of 1353, 0.754619364375462\n",
      "Done 1022 out of 1353, 0.7553584626755359\n",
      "Done 1023 out of 1353, 0.7560975609756098\n",
      "Done 1024 out of 1353, 0.7568366592756837\n",
      "Done 1025 out of 1353, 0.7575757575757576\n",
      "Done 1026 out of 1353, 0.7583148558758315\n",
      "Done 1027 out of 1353, 0.7590539541759054\n",
      "Done 1028 out of 1353, 0.7597930524759793\n",
      "Done 1029 out of 1353, 0.7605321507760532\n",
      "Done 1030 out of 1353, 0.7612712490761271\n",
      "Done 1031 out of 1353, 0.762010347376201\n",
      "Done 1032 out of 1353, 0.7627494456762749\n",
      "Done 1033 out of 1353, 0.7634885439763488\n",
      "Done 1034 out of 1353, 0.7642276422764228\n",
      "Done 1035 out of 1353, 0.7649667405764967\n",
      "Done 1036 out of 1353, 0.7657058388765706\n",
      "Done 1037 out of 1353, 0.7664449371766445\n",
      "Done 1038 out of 1353, 0.7671840354767184\n",
      "Done 1039 out of 1353, 0.7679231337767923\n",
      "Done 1040 out of 1353, 0.7686622320768662\n",
      "Done 1041 out of 1353, 0.7694013303769401\n",
      "Done 1042 out of 1353, 0.770140428677014\n",
      "Done 1043 out of 1353, 0.770879526977088\n",
      "Done 1044 out of 1353, 0.7716186252771619\n",
      "Done 1045 out of 1353, 0.7723577235772358\n",
      "Done 1046 out of 1353, 0.7730968218773097\n",
      "Done 1047 out of 1353, 0.7738359201773836\n",
      "Done 1048 out of 1353, 0.7745750184774575\n",
      "Done 1049 out of 1353, 0.7753141167775314\n",
      "Done 1050 out of 1353, 0.7760532150776053\n",
      "Done 1051 out of 1353, 0.7767923133776792\n",
      "Done 1052 out of 1353, 0.7775314116777532\n",
      "Done 1053 out of 1353, 0.7782705099778271\n",
      "Done 1054 out of 1353, 0.779009608277901\n",
      "Done 1055 out of 1353, 0.7797487065779749\n",
      "Done 1056 out of 1353, 0.7804878048780488\n",
      "Done 1057 out of 1353, 0.7812269031781227\n",
      "Done 1058 out of 1353, 0.7819660014781966\n",
      "Done 1059 out of 1353, 0.7827050997782705\n",
      "Done 1060 out of 1353, 0.7834441980783444\n",
      "Done 1061 out of 1353, 0.7841832963784183\n",
      "Done 1062 out of 1353, 0.7849223946784922\n",
      "Done 1063 out of 1353, 0.7856614929785661\n",
      "Done 1064 out of 1353, 0.78640059127864\n",
      "Done 1065 out of 1353, 0.7871396895787139\n",
      "Done 1066 out of 1353, 0.7878787878787878\n",
      "Done 1067 out of 1353, 0.7886178861788617\n",
      "Done 1068 out of 1353, 0.7893569844789357\n",
      "Done 1069 out of 1353, 0.7900960827790096\n",
      "Done 1070 out of 1353, 0.7908351810790836\n",
      "Done 1071 out of 1353, 0.7915742793791575\n",
      "Done 1072 out of 1353, 0.7923133776792314\n",
      "Done 1073 out of 1353, 0.7930524759793053\n",
      "Done 1074 out of 1353, 0.7937915742793792\n",
      "Done 1075 out of 1353, 0.7945306725794531\n",
      "Done 1076 out of 1353, 0.795269770879527\n",
      "Done 1077 out of 1353, 0.7960088691796009\n",
      "Done 1078 out of 1353, 0.7967479674796748\n",
      "Done 1079 out of 1353, 0.7974870657797487\n",
      "Done 1080 out of 1353, 0.7982261640798226\n",
      "Done 1081 out of 1353, 0.7989652623798965\n",
      "Done 1082 out of 1353, 0.7997043606799704\n",
      "Done 1083 out of 1353, 0.8004434589800443\n",
      "Done 1084 out of 1353, 0.8011825572801182\n",
      "Done 1085 out of 1353, 0.8019216555801921\n",
      "Done 1086 out of 1353, 0.802660753880266\n",
      "Done 1087 out of 1353, 0.8033998521803399\n",
      "Done 1088 out of 1353, 0.804138950480414\n",
      "Done 1089 out of 1353, 0.8048780487804879\n",
      "Done 1090 out of 1353, 0.8056171470805618\n",
      "Done 1091 out of 1353, 0.8063562453806357\n",
      "Done 1092 out of 1353, 0.8070953436807096\n",
      "Done 1093 out of 1353, 0.8078344419807835\n",
      "Done 1094 out of 1353, 0.8085735402808574\n",
      "Done 1095 out of 1353, 0.8093126385809313\n",
      "Done 1096 out of 1353, 0.8100517368810052\n",
      "Done 1097 out of 1353, 0.8107908351810791\n",
      "Done 1098 out of 1353, 0.811529933481153\n",
      "Done 1099 out of 1353, 0.8122690317812269\n",
      "Done 1100 out of 1353, 0.8130081300813008\n",
      "Done 1101 out of 1353, 0.8137472283813747\n",
      "Done 1102 out of 1353, 0.8144863266814486\n",
      "Done 1103 out of 1353, 0.8152254249815225\n",
      "Done 1104 out of 1353, 0.8159645232815964\n",
      "Done 1105 out of 1353, 0.8167036215816703\n",
      "Done 1106 out of 1353, 0.8174427198817442\n",
      "Done 1107 out of 1353, 0.8181818181818182\n",
      "Done 1108 out of 1353, 0.8189209164818921\n",
      "Done 1109 out of 1353, 0.819660014781966\n",
      "Done 1110 out of 1353, 0.8203991130820399\n",
      "Done 1111 out of 1353, 0.8211382113821138\n",
      "Done 1112 out of 1353, 0.8218773096821878\n",
      "Done 1113 out of 1353, 0.8226164079822617\n",
      "Done 1114 out of 1353, 0.8233555062823356\n",
      "Done 1115 out of 1353, 0.8240946045824095\n",
      "Done 1116 out of 1353, 0.8248337028824834\n",
      "Done 1117 out of 1353, 0.8255728011825573\n",
      "Done 1118 out of 1353, 0.8263118994826312\n",
      "Done 1119 out of 1353, 0.8270509977827051\n",
      "Done 1120 out of 1353, 0.827790096082779\n",
      "Done 1121 out of 1353, 0.8285291943828529\n",
      "Done 1122 out of 1353, 0.8292682926829268\n",
      "Done 1123 out of 1353, 0.8300073909830007\n",
      "Done 1124 out of 1353, 0.8307464892830746\n",
      "Done 1125 out of 1353, 0.8314855875831486\n",
      "Done 1126 out of 1353, 0.8322246858832225\n",
      "Done 1127 out of 1353, 0.8329637841832964\n",
      "Done 1128 out of 1353, 0.8337028824833703\n",
      "Done 1129 out of 1353, 0.8344419807834442\n",
      "Done 1130 out of 1353, 0.8351810790835181\n",
      "Done 1131 out of 1353, 0.835920177383592\n",
      "Done 1132 out of 1353, 0.8366592756836659\n",
      "Done 1133 out of 1353, 0.8373983739837398\n",
      "Done 1134 out of 1353, 0.8381374722838137\n",
      "Done 1135 out of 1353, 0.8388765705838876\n",
      "Done 1136 out of 1353, 0.8396156688839616\n",
      "Done 1137 out of 1353, 0.8403547671840355\n",
      "Done 1138 out of 1353, 0.8410938654841094\n",
      "Done 1139 out of 1353, 0.8418329637841833\n",
      "Done 1140 out of 1353, 0.8425720620842572\n",
      "Done 1141 out of 1353, 0.8433111603843311\n",
      "Done 1142 out of 1353, 0.844050258684405\n",
      "Done 1143 out of 1353, 0.844789356984479\n",
      "Done 1144 out of 1353, 0.8455284552845529\n",
      "Done 1145 out of 1353, 0.8462675535846268\n",
      "Done 1146 out of 1353, 0.8470066518847007\n",
      "Done 1147 out of 1353, 0.8477457501847746\n",
      "Done 1148 out of 1353, 0.8484848484848485\n",
      "Done 1149 out of 1353, 0.8492239467849224\n",
      "Done 1150 out of 1353, 0.8499630450849963\n",
      "Done 1151 out of 1353, 0.8507021433850702\n",
      "Done 1152 out of 1353, 0.8514412416851441\n",
      "Done 1153 out of 1353, 0.852180339985218\n",
      "Done 1154 out of 1353, 0.8529194382852919\n",
      "Done 1155 out of 1353, 0.8536585365853658\n",
      "Done 1156 out of 1353, 0.8543976348854397\n",
      "Done 1157 out of 1353, 0.8551367331855136\n",
      "Done 1158 out of 1353, 0.8558758314855875\n",
      "Done 1159 out of 1353, 0.8566149297856614\n",
      "Done 1160 out of 1353, 0.8573540280857354\n",
      "Done 1161 out of 1353, 0.8580931263858093\n",
      "Done 1162 out of 1353, 0.8588322246858833\n",
      "Done 1163 out of 1353, 0.8595713229859572\n",
      "Done 1164 out of 1353, 0.8603104212860311\n",
      "Done 1165 out of 1353, 0.861049519586105\n",
      "Done 1166 out of 1353, 0.8617886178861789\n",
      "Done 1167 out of 1353, 0.8625277161862528\n",
      "Done 1168 out of 1353, 0.8632668144863267\n",
      "Done 1169 out of 1353, 0.8640059127864006\n",
      "Done 1170 out of 1353, 0.8647450110864745\n",
      "Done 1171 out of 1353, 0.8654841093865484\n",
      "Done 1172 out of 1353, 0.8662232076866223\n",
      "Done 1173 out of 1353, 0.8669623059866962\n",
      "Done 1174 out of 1353, 0.8677014042867701\n",
      "Done 1175 out of 1353, 0.868440502586844\n",
      "Done 1176 out of 1353, 0.8691796008869179\n",
      "Done 1177 out of 1353, 0.8699186991869918\n",
      "Done 1178 out of 1353, 0.8706577974870657\n",
      "Done 1179 out of 1353, 0.8713968957871396\n",
      "Done 1180 out of 1353, 0.8721359940872137\n",
      "Done 1181 out of 1353, 0.8728750923872876\n",
      "Done 1182 out of 1353, 0.8736141906873615\n",
      "Done 1183 out of 1353, 0.8743532889874354\n",
      "Done 1184 out of 1353, 0.8750923872875093\n",
      "Done 1185 out of 1353, 0.8758314855875832\n",
      "Done 1186 out of 1353, 0.8765705838876571\n",
      "Done 1187 out of 1353, 0.877309682187731\n",
      "Done 1188 out of 1353, 0.8780487804878049\n",
      "Done 1189 out of 1353, 0.8787878787878788\n",
      "Done 1190 out of 1353, 0.8795269770879527\n",
      "Done 1191 out of 1353, 0.8802660753880266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1192 out of 1353, 0.8810051736881005\n",
      "Done 1193 out of 1353, 0.8817442719881744\n",
      "Done 1194 out of 1353, 0.8824833702882483\n",
      "Done 1195 out of 1353, 0.8832224685883222\n",
      "Done 1196 out of 1353, 0.8839615668883961\n",
      "Done 1197 out of 1353, 0.88470066518847\n",
      "Done 1198 out of 1353, 0.885439763488544\n",
      "Done 1199 out of 1353, 0.8861788617886179\n",
      "Done 1200 out of 1353, 0.8869179600886918\n",
      "Done 1201 out of 1353, 0.8876570583887657\n",
      "Done 1202 out of 1353, 0.8883961566888396\n",
      "Done 1203 out of 1353, 0.8891352549889135\n",
      "Done 1204 out of 1353, 0.8898743532889875\n",
      "Done 1205 out of 1353, 0.8906134515890614\n",
      "Done 1206 out of 1353, 0.8913525498891353\n",
      "Done 1207 out of 1353, 0.8920916481892092\n",
      "Done 1208 out of 1353, 0.8928307464892831\n",
      "Done 1209 out of 1353, 0.893569844789357\n",
      "Done 1210 out of 1353, 0.8943089430894309\n",
      "Done 1211 out of 1353, 0.8950480413895048\n",
      "Done 1212 out of 1353, 0.8957871396895787\n",
      "Done 1213 out of 1353, 0.8965262379896526\n",
      "Done 1214 out of 1353, 0.8972653362897265\n",
      "Done 1215 out of 1353, 0.8980044345898004\n",
      "Done 1216 out of 1353, 0.8987435328898744\n",
      "Done 1217 out of 1353, 0.8994826311899483\n",
      "Done 1218 out of 1353, 0.9002217294900222\n",
      "Done 1219 out of 1353, 0.9009608277900961\n",
      "Done 1220 out of 1353, 0.90169992609017\n",
      "Done 1221 out of 1353, 0.9024390243902439\n",
      "Done 1222 out of 1353, 0.9031781226903178\n",
      "Done 1223 out of 1353, 0.9039172209903917\n",
      "Done 1224 out of 1353, 0.9046563192904656\n",
      "Done 1225 out of 1353, 0.9053954175905395\n",
      "Done 1226 out of 1353, 0.9061345158906134\n",
      "Done 1227 out of 1353, 0.9068736141906873\n",
      "Done 1228 out of 1353, 0.9076127124907613\n",
      "Done 1229 out of 1353, 0.9083518107908352\n",
      "Done 1230 out of 1353, 0.9090909090909091\n",
      "Done 1231 out of 1353, 0.909830007390983\n",
      "Done 1232 out of 1353, 0.9105691056910569\n",
      "Done 1233 out of 1353, 0.9113082039911308\n",
      "Done 1234 out of 1353, 0.9120473022912047\n",
      "Done 1235 out of 1353, 0.9127864005912787\n",
      "Done 1236 out of 1353, 0.9135254988913526\n",
      "Done 1237 out of 1353, 0.9142645971914265\n",
      "Done 1238 out of 1353, 0.9150036954915004\n",
      "Done 1239 out of 1353, 0.9157427937915743\n",
      "Done 1240 out of 1353, 0.9164818920916482\n",
      "Done 1241 out of 1353, 0.9172209903917221\n",
      "Done 1242 out of 1353, 0.917960088691796\n",
      "Done 1243 out of 1353, 0.9186991869918699\n",
      "Done 1244 out of 1353, 0.9194382852919438\n",
      "Done 1245 out of 1353, 0.9201773835920177\n",
      "Done 1246 out of 1353, 0.9209164818920916\n",
      "Done 1247 out of 1353, 0.9216555801921655\n",
      "Done 1248 out of 1353, 0.9223946784922394\n",
      "Done 1249 out of 1353, 0.9231337767923133\n",
      "Done 1250 out of 1353, 0.9238728750923872\n",
      "Done 1251 out of 1353, 0.9246119733924612\n",
      "Done 1252 out of 1353, 0.925351071692535\n",
      "Done 1253 out of 1353, 0.9260901699926091\n",
      "Done 1254 out of 1353, 0.926829268292683\n",
      "Done 1255 out of 1353, 0.9275683665927569\n",
      "Done 1256 out of 1353, 0.9283074648928308\n",
      "Done 1257 out of 1353, 0.9290465631929047\n",
      "Done 1258 out of 1353, 0.9297856614929786\n",
      "Done 1259 out of 1353, 0.9305247597930525\n",
      "Done 1260 out of 1353, 0.9312638580931264\n",
      "Done 1261 out of 1353, 0.9320029563932003\n",
      "Done 1262 out of 1353, 0.9327420546932742\n",
      "Done 1263 out of 1353, 0.9334811529933481\n",
      "Done 1264 out of 1353, 0.934220251293422\n",
      "Done 1265 out of 1353, 0.9349593495934959\n",
      "Done 1266 out of 1353, 0.9356984478935698\n",
      "Done 1267 out of 1353, 0.9364375461936437\n",
      "Done 1268 out of 1353, 0.9371766444937176\n",
      "Done 1269 out of 1353, 0.9379157427937915\n",
      "Done 1270 out of 1353, 0.9386548410938654\n",
      "Done 1271 out of 1353, 0.9393939393939394\n",
      "Done 1272 out of 1353, 0.9401330376940134\n",
      "Done 1273 out of 1353, 0.9408721359940873\n",
      "Done 1274 out of 1353, 0.9416112342941612\n",
      "Done 1275 out of 1353, 0.9423503325942351\n",
      "Done 1276 out of 1353, 0.943089430894309\n",
      "Done 1277 out of 1353, 0.9438285291943829\n",
      "Done 1278 out of 1353, 0.9445676274944568\n",
      "Done 1279 out of 1353, 0.9453067257945307\n",
      "Done 1280 out of 1353, 0.9460458240946046\n",
      "Done 1281 out of 1353, 0.9467849223946785\n",
      "Done 1282 out of 1353, 0.9475240206947524\n",
      "Done 1283 out of 1353, 0.9482631189948263\n",
      "Done 1284 out of 1353, 0.9490022172949002\n",
      "Done 1285 out of 1353, 0.9497413155949741\n",
      "Done 1286 out of 1353, 0.950480413895048\n",
      "Done 1287 out of 1353, 0.9512195121951219\n",
      "Done 1288 out of 1353, 0.9519586104951958\n",
      "Done 1289 out of 1353, 0.9526977087952697\n",
      "Done 1290 out of 1353, 0.9534368070953437\n",
      "Done 1291 out of 1353, 0.9541759053954176\n",
      "Done 1292 out of 1353, 0.9549150036954915\n",
      "Done 1293 out of 1353, 0.9556541019955654\n",
      "Done 1294 out of 1353, 0.9563932002956393\n",
      "Done 1295 out of 1353, 0.9571322985957132\n",
      "Done 1296 out of 1353, 0.9578713968957872\n",
      "Done 1297 out of 1353, 0.9586104951958611\n",
      "Done 1298 out of 1353, 0.959349593495935\n",
      "Done 1299 out of 1353, 0.9600886917960089\n",
      "Done 1300 out of 1353, 0.9608277900960828\n",
      "Done 1301 out of 1353, 0.9615668883961567\n",
      "Done 1302 out of 1353, 0.9623059866962306\n",
      "Done 1303 out of 1353, 0.9630450849963045\n",
      "Done 1304 out of 1353, 0.9637841832963784\n",
      "Done 1305 out of 1353, 0.9645232815964523\n",
      "Done 1306 out of 1353, 0.9652623798965262\n",
      "Done 1307 out of 1353, 0.9660014781966001\n",
      "Done 1308 out of 1353, 0.9667405764966741\n",
      "Done 1309 out of 1353, 0.967479674796748\n",
      "Done 1310 out of 1353, 0.9682187730968219\n",
      "Done 1311 out of 1353, 0.9689578713968958\n",
      "Done 1312 out of 1353, 0.9696969696969697\n",
      "Done 1313 out of 1353, 0.9704360679970436\n",
      "Done 1314 out of 1353, 0.9711751662971175\n",
      "Done 1315 out of 1353, 0.9719142645971914\n",
      "Done 1316 out of 1353, 0.9726533628972653\n",
      "Done 1317 out of 1353, 0.9733924611973392\n",
      "Done 1318 out of 1353, 0.9741315594974131\n",
      "Done 1319 out of 1353, 0.974870657797487\n",
      "Done 1320 out of 1353, 0.975609756097561\n",
      "Done 1321 out of 1353, 0.9763488543976349\n",
      "Done 1322 out of 1353, 0.9770879526977088\n",
      "Done 1323 out of 1353, 0.9778270509977827\n",
      "Done 1324 out of 1353, 0.9785661492978566\n",
      "Done 1325 out of 1353, 0.9793052475979305\n",
      "Done 1326 out of 1353, 0.9800443458980045\n",
      "Done 1327 out of 1353, 0.9807834441980784\n",
      "Done 1328 out of 1353, 0.9815225424981523\n",
      "Done 1329 out of 1353, 0.9822616407982262\n",
      "Done 1330 out of 1353, 0.9830007390983001\n",
      "Done 1331 out of 1353, 0.983739837398374\n",
      "Done 1332 out of 1353, 0.9844789356984479\n",
      "Done 1333 out of 1353, 0.9852180339985218\n",
      "Done 1334 out of 1353, 0.9859571322985957\n",
      "Done 1335 out of 1353, 0.9866962305986696\n",
      "Done 1336 out of 1353, 0.9874353288987435\n",
      "Done 1337 out of 1353, 0.9881744271988174\n",
      "Done 1338 out of 1353, 0.9889135254988913\n",
      "Done 1339 out of 1353, 0.9896526237989652\n",
      "Done 1340 out of 1353, 0.9903917220990391\n",
      "Done 1341 out of 1353, 0.991130820399113\n",
      "Done 1342 out of 1353, 0.991869918699187\n",
      "Done 1343 out of 1353, 0.9926090169992609\n",
      "Done 1344 out of 1353, 0.9933481152993349\n",
      "Done 1345 out of 1353, 0.9940872135994088\n",
      "Done 1346 out of 1353, 0.9948263118994827\n",
      "Done 1347 out of 1353, 0.9955654101995566\n",
      "Done 1348 out of 1353, 0.9963045084996305\n",
      "Done 1349 out of 1353, 0.9970436067997044\n",
      "Done 1350 out of 1353, 0.9977827050997783\n",
      "Done 1351 out of 1353, 0.9985218033998522\n",
      "Done 1352 out of 1353, 0.9992609016999261\n",
      "Done 1353 out of 1353, 1.0\n",
      "Done 1 out of 134, 0.007462686567164179\n",
      "Done 2 out of 134, 0.014925373134328358\n",
      "Done 3 out of 134, 0.022388059701492536\n",
      "Done 4 out of 134, 0.029850746268656716\n",
      "Done 5 out of 134, 0.03731343283582089\n",
      "Done 6 out of 134, 0.04477611940298507\n",
      "Done 7 out of 134, 0.05223880597014925\n",
      "Done 8 out of 134, 0.05970149253731343\n",
      "Done 9 out of 134, 0.06716417910447761\n",
      "Done 10 out of 134, 0.07462686567164178\n",
      "Done 11 out of 134, 0.08208955223880597\n",
      "Done 12 out of 134, 0.08955223880597014\n",
      "Done 13 out of 134, 0.09701492537313433\n",
      "Done 14 out of 134, 0.1044776119402985\n",
      "Done 15 out of 134, 0.11194029850746269\n",
      "Done 16 out of 134, 0.11940298507462686\n",
      "Done 17 out of 134, 0.12686567164179105\n",
      "Done 18 out of 134, 0.13432835820895522\n",
      "Done 19 out of 134, 0.1417910447761194\n",
      "Done 20 out of 134, 0.14925373134328357\n",
      "Done 21 out of 134, 0.15671641791044777\n",
      "Done 22 out of 134, 0.16417910447761194\n",
      "Done 23 out of 134, 0.17164179104477612\n",
      "Done 24 out of 134, 0.1791044776119403\n",
      "Done 25 out of 134, 0.1865671641791045\n",
      "Done 26 out of 134, 0.19402985074626866\n",
      "Done 27 out of 134, 0.20149253731343283\n",
      "Done 28 out of 134, 0.208955223880597\n",
      "Done 29 out of 134, 0.21641791044776118\n",
      "Done 30 out of 134, 0.22388059701492538\n",
      "Done 31 out of 134, 0.23134328358208955\n",
      "Done 32 out of 134, 0.23880597014925373\n",
      "Done 33 out of 134, 0.2462686567164179\n",
      "Done 34 out of 134, 0.2537313432835821\n",
      "Done 35 out of 134, 0.26119402985074625\n",
      "Done 36 out of 134, 0.26865671641791045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 37 out of 134, 0.27611940298507465\n",
      "Done 38 out of 134, 0.2835820895522388\n",
      "Done 39 out of 134, 0.291044776119403\n",
      "Done 40 out of 134, 0.29850746268656714\n",
      "Done 41 out of 134, 0.30597014925373134\n",
      "Done 42 out of 134, 0.31343283582089554\n",
      "Done 43 out of 134, 0.3208955223880597\n",
      "Done 44 out of 134, 0.3283582089552239\n",
      "Done 45 out of 134, 0.3358208955223881\n",
      "Done 46 out of 134, 0.34328358208955223\n",
      "Done 47 out of 134, 0.35074626865671643\n",
      "Done 48 out of 134, 0.3582089552238806\n",
      "Done 49 out of 134, 0.3656716417910448\n",
      "Done 50 out of 134, 0.373134328358209\n",
      "Done 51 out of 134, 0.3805970149253731\n",
      "Done 52 out of 134, 0.3880597014925373\n",
      "Done 53 out of 134, 0.39552238805970147\n",
      "Done 54 out of 134, 0.40298507462686567\n",
      "Done 55 out of 134, 0.41044776119402987\n",
      "Done 56 out of 134, 0.417910447761194\n",
      "Done 57 out of 134, 0.4253731343283582\n",
      "Done 58 out of 134, 0.43283582089552236\n",
      "Done 59 out of 134, 0.44029850746268656\n",
      "Done 60 out of 134, 0.44776119402985076\n",
      "Done 61 out of 134, 0.4552238805970149\n",
      "Done 62 out of 134, 0.4626865671641791\n",
      "Done 63 out of 134, 0.4701492537313433\n",
      "Done 64 out of 134, 0.47761194029850745\n",
      "Done 65 out of 134, 0.48507462686567165\n",
      "Done 66 out of 134, 0.4925373134328358\n",
      "Done 67 out of 134, 0.5\n",
      "Done 68 out of 134, 0.5074626865671642\n",
      "Done 69 out of 134, 0.5149253731343284\n",
      "Done 70 out of 134, 0.5223880597014925\n",
      "Done 71 out of 134, 0.5298507462686567\n",
      "Done 72 out of 134, 0.5373134328358209\n",
      "Done 73 out of 134, 0.5447761194029851\n",
      "Done 74 out of 134, 0.5522388059701493\n",
      "Done 75 out of 134, 0.5597014925373134\n",
      "Done 76 out of 134, 0.5671641791044776\n",
      "Done 77 out of 134, 0.5746268656716418\n",
      "Done 78 out of 134, 0.582089552238806\n",
      "Done 79 out of 134, 0.5895522388059702\n",
      "Done 80 out of 134, 0.5970149253731343\n",
      "Done 81 out of 134, 0.6044776119402985\n",
      "Done 82 out of 134, 0.6119402985074627\n",
      "Done 83 out of 134, 0.6194029850746269\n",
      "Done 84 out of 134, 0.6268656716417911\n",
      "Done 85 out of 134, 0.6343283582089553\n",
      "Done 86 out of 134, 0.6417910447761194\n",
      "Done 87 out of 134, 0.6492537313432836\n",
      "Done 88 out of 134, 0.6567164179104478\n",
      "Done 89 out of 134, 0.664179104477612\n",
      "Done 90 out of 134, 0.6716417910447762\n",
      "Done 91 out of 134, 0.6791044776119403\n",
      "Done 92 out of 134, 0.6865671641791045\n",
      "Done 93 out of 134, 0.6940298507462687\n",
      "Done 94 out of 134, 0.7014925373134329\n",
      "Done 95 out of 134, 0.7089552238805971\n",
      "Done 96 out of 134, 0.7164179104477612\n",
      "Done 97 out of 134, 0.7238805970149254\n",
      "Done 98 out of 134, 0.7313432835820896\n",
      "Done 99 out of 134, 0.7388059701492538\n",
      "Done 100 out of 134, 0.746268656716418\n",
      "Done 101 out of 134, 0.753731343283582\n",
      "Done 102 out of 134, 0.7611940298507462\n",
      "Done 103 out of 134, 0.7686567164179104\n",
      "Done 104 out of 134, 0.7761194029850746\n",
      "Done 105 out of 134, 0.7835820895522388\n",
      "Done 106 out of 134, 0.7910447761194029\n",
      "Done 107 out of 134, 0.7985074626865671\n",
      "Done 108 out of 134, 0.8059701492537313\n",
      "Done 109 out of 134, 0.8134328358208955\n",
      "Done 110 out of 134, 0.8208955223880597\n",
      "Done 111 out of 134, 0.8283582089552238\n",
      "Done 112 out of 134, 0.835820895522388\n",
      "Done 113 out of 134, 0.8432835820895522\n",
      "Done 114 out of 134, 0.8507462686567164\n",
      "Done 115 out of 134, 0.8582089552238806\n",
      "Done 116 out of 134, 0.8656716417910447\n",
      "Done 117 out of 134, 0.8731343283582089\n",
      "Done 118 out of 134, 0.8805970149253731\n",
      "Done 119 out of 134, 0.8880597014925373\n",
      "Done 120 out of 134, 0.8955223880597015\n",
      "Done 121 out of 134, 0.9029850746268657\n",
      "Done 122 out of 134, 0.9104477611940298\n",
      "Done 123 out of 134, 0.917910447761194\n",
      "Done 124 out of 134, 0.9253731343283582\n",
      "Done 125 out of 134, 0.9328358208955224\n",
      "Done 126 out of 134, 0.9402985074626866\n",
      "Done 127 out of 134, 0.9477611940298507\n",
      "Done 128 out of 134, 0.9552238805970149\n",
      "Done 129 out of 134, 0.9626865671641791\n",
      "Done 130 out of 134, 0.9701492537313433\n",
      "Done 131 out of 134, 0.9776119402985075\n",
      "Done 132 out of 134, 0.9850746268656716\n",
      "Done 133 out of 134, 0.9925373134328358\n",
      "Done 134 out of 134, 1.0\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = prepare_data(train, 0.05, 0.025)\n",
    "x_test, y_test = prepare_data(test, 0.05, 0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training and our information we define the recall, precision, f-score and precision metrics. Those allow us quickly to evaluate how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def nist(y_true, y_pred):\n",
    "    predicted = K.round(K.clip(y_pred, 0, 1))\n",
    "    return K.sum(K.abs(predicted-y_true))/K.sum(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple models that we can create for this problem. Noticeably, the features extracted above imply that the data is timeseries one. Hence there are two options that we can think of:\n",
    "- a simple fully connected layer\n",
    "- a more complex LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple fully connected model expects an input of size 20 by 34 and considers the current and next words. It implements dropout in order to reduce overfitting. Uses a standard optimizer that is adam and as a loss function binary entropy that is a typical function for a binay classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(number_of_deep_layers, size_of_deep_layers):\n",
    "    features_input = Input(shape=(20, 34))\n",
    "    features_next_input = Input(shape=(20, 34))\n",
    "    pause = Input(shape=(20, 34))\n",
    "    \n",
    "    x = Concatenate()([features_input, features_next_input, pause])\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    for i in range(number_of_deep_layers):\n",
    "        x = Dense(size_of_deep_layers, activation='relu')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[features_input, features_next_input, pause], outputs=x)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', f1_m, precision_m, recall_m, nist])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complex model is the LSTM model. The idea for using it is that it is usually used in similar timeseries problems and could nicely integrate into the current one. The network would encode the information from the current and next words into a one smaller vector such that we can use it in a simple fully connected layer to conduct infrence. The inputs are a bit different than for the fully connected layer. Because in theory LSTMs can accept varying length of input, we do not specify the input size fully. One thing to notice is that the input data for the next word should be rotated. This idea in thory would make sure that we forget more the parts of the word that are further from the break and focus more on the ones that are closer to it.An important implementation detail is the usage of CuDNN LSTM's. Those are far faster and converge much nicely than their native tensorflow implementation. Training without those took very long and the network required loads of epochs to converge properly. However, those also impose some condierable constraints on what we can do with that layer, e.g. it's activation function is prespecfied. In order to reduce the overfitting we add dropout layers with small rate of 0,1. Once again we use adam with binary crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(LSTM_size, number_of_deep_layers, size_of_deep_layers):\n",
    "    features_input = Input(shape=(None, 34))\n",
    "    features_flipped_input = Input(shape=(None, 34))\n",
    "\n",
    "    x = CuDNNLSTM(LSTM_size)(features_input, )\n",
    "    current_word = Dropout(0.1)(x)\n",
    "    \n",
    "    x = CuDNNLSTM(LSTM_size)(features_flipped_input)\n",
    "    next_word = Dropout(0.1)(x)\n",
    "    \n",
    "    pause_input = Input(shape=(1,))\n",
    "    x = Concatenate()([current_word, next_word, pause_input])\n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    for i in range(number_of_deep_layers):\n",
    "        x = Dense(size_of_deep_layers, activation='relu')(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[features_input, features_flipped_input, pause_input], outputs=x)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', f1_m, precision_m, recall_m, nist])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important step before we can conduct any training is that we need to scale the data to increase the speed of learning and also pad it. The padding is required because, even though, LSTMs in theory can be of varying length in reality the implementation, in order to be fast, must have a constant length of input vector. Also the fully connected layer requires it. We use the standard scaler and pad the sequences to length of 20. We also flip the word features as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_pad(data, features_scaler=None, pause_scaler=None):\n",
    "    if features_scaler == None and pause_scaler == None:\n",
    "        features_scaler = StandardScaler()\n",
    "        pause_scaler = StandardScaler()\n",
    "\n",
    "        pause_normalised = pause_scaler.fit_transform(np.array(data[1]).reshape(-1, 1))\n",
    "        features_scaler.fit(np.vstack(data[0]))\n",
    "    else:\n",
    "        pause_normalised = pause_scaler.transform(np.array(data[1]).reshape(-1, 1))\n",
    "        \n",
    "    features_normalised = []\n",
    "\n",
    "    for feature in data[0]: \n",
    "        features_normalised.append(features_scaler.transform(feature))\n",
    "\n",
    "    features_padded = pad_sequences(features_normalised, dtype='float32', maxlen=20)\n",
    "\n",
    "    flipped = []\n",
    "    for element in data[0][1:]:\n",
    "        flipped.append(np.flip(element, 0))\n",
    "\n",
    "    features_normalised_flipped = []\n",
    "\n",
    "    for feature in flipped: \n",
    "        features_normalised_flipped.append(features_scaler.transform(feature))\n",
    "\n",
    "    features_normalised_flipped.append(np.zeros(features_normalised_flipped[-1].shape))\n",
    "    features_flipped_padded = pad_sequences(features_normalised_flipped, dtype='float32', maxlen=20)\n",
    "\n",
    "    return [features_padded, features_flipped_padded, pause_normalised], features_scaler, pause_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the above code to obtain the new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, features_scaler, pause_scaler = scale_pad(x_train)\n",
    "x_ts, _, _ = scale_pad(x_test, features_scaler, pause_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to allow varying experiments we for loop through the possible sizes of various parameters. We use checkpoints, tensorboard and the early stopping that can be used in longer experiments to automate finishing of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61237 samples, validate on 6792 samples\n",
      "Epoch 1/1000\n",
      "61237/61237 [==============================] - 3s 48us/step - loss: 0.2378 - acc: 0.9451 - f1_m: 0.0042 - precision_m: 0.0083 - recall_m: 0.0171 - nist: 1.6464 - val_loss: 0.1352 - val_acc: 0.9616 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00 - val_nist: 1.0000\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.00000, saving model to training/BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-32-number-hidden-3-size-hidden-80-04_09_2019-02_04_26.h5\n",
      "Epoch 2/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.1230 - acc: 0.9659 - f1_m: 0.0858 - precision_m: 0.4361 - recall_m: 0.0493 - nist: 0.9764 - val_loss: 0.1210 - val_acc: 0.9650 - val_f1_m: 0.2374 - val_precision_m: 0.7521 - val_recall_m: 0.1447 - val_nist: 0.9128\n",
      "\n",
      "Epoch 00002: val_f1_m improved from 0.00000 to 0.23739, saving model to training/BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-32-number-hidden-3-size-hidden-80-04_09_2019-02_04_26.h5\n",
      "Epoch 3/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.1140 - acc: 0.9677 - f1_m: 0.2514 - precision_m: 0.6671 - recall_m: 0.1578 - nist: 0.9246 - val_loss: 0.1129 - val_acc: 0.9660 - val_f1_m: 0.3335 - val_precision_m: 0.6896 - val_recall_m: 0.2237 - val_nist: 0.8859\n",
      "\n",
      "Epoch 00003: val_f1_m improved from 0.23739 to 0.33353, saving model to training/BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-32-number-hidden-3-size-hidden-80-04_09_2019-02_04_26.h5\n",
      "Epoch 4/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.1074 - acc: 0.9685 - f1_m: 0.3095 - precision_m: 0.6585 - recall_m: 0.2071 - nist: 0.9069 - val_loss: 0.1099 - val_acc: 0.9673 - val_f1_m: 0.3897 - val_precision_m: 0.6886 - val_recall_m: 0.2777 - val_nist: 0.8544\n",
      "\n",
      "Epoch 00004: val_f1_m improved from 0.33353 to 0.38967, saving model to training/BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-32-number-hidden-3-size-hidden-80-04_09_2019-02_04_26.h5\n",
      "Epoch 5/1000\n",
      "61237/61237 [==============================] - 1s 17us/step - loss: 0.1043 - acc: 0.9692 - f1_m: 0.3365 - precision_m: 0.6723 - recall_m: 0.2314 - nist: 0.8843 - val_loss: 0.1061 - val_acc: 0.9691 - val_f1_m: 0.4626 - val_precision_m: 0.6908 - val_recall_m: 0.3518 - val_nist: 0.8094\n",
      "\n",
      "Epoch 00005: val_f1_m improved from 0.38967 to 0.46260, saving model to training/BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-32-number-hidden-3-size-hidden-80-04_09_2019-02_04_26.h5\n",
      "Epoch 6/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.1001 - acc: 0.9700 - f1_m: 0.3891 - precision_m: 0.6725 - recall_m: 0.2788 - nist: 0.8627 - val_loss: 0.1057 - val_acc: 0.9683 - val_f1_m: 0.4702 - val_precision_m: 0.6439 - val_recall_m: 0.3716 - val_nist: 0.8332\n",
      "\n",
      "Epoch 00006: val_f1_m improved from 0.46260 to 0.47022, saving model to training/BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-32-number-hidden-3-size-hidden-80-04_09_2019-02_04_26.h5\n",
      "Epoch 7/1000\n",
      "61237/61237 [==============================] - 1s 18us/step - loss: 0.0979 - acc: 0.9701 - f1_m: 0.4189 - precision_m: 0.6535 - recall_m: 0.3146 - nist: 0.8598 - val_loss: 0.1034 - val_acc: 0.9691 - val_f1_m: 0.4883 - val_precision_m: 0.6537 - val_recall_m: 0.3914 - val_nist: 0.8134\n",
      "\n",
      "Epoch 00007: val_f1_m improved from 0.47022 to 0.48835, saving model to training/BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-32-number-hidden-3-size-hidden-80-04_09_2019-02_04_26.h5\n",
      "Epoch 8/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0964 - acc: 0.9704 - f1_m: 0.4403 - precision_m: 0.6463 - recall_m: 0.3399 - nist: 0.8499 - val_loss: 0.1033 - val_acc: 0.9701 - val_f1_m: 0.5185 - val_precision_m: 0.6678 - val_recall_m: 0.4256 - val_nist: 0.7859\n",
      "\n",
      "Epoch 00008: val_f1_m improved from 0.48835 to 0.51848, saving model to training/BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-32-number-hidden-3-size-hidden-80-04_09_2019-02_04_26.h5\n",
      "Epoch 9/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0934 - acc: 0.9714 - f1_m: 0.4784 - precision_m: 0.6565 - recall_m: 0.3831 - nist: 0.8220 - val_loss: 0.1024 - val_acc: 0.9707 - val_f1_m: 0.5539 - val_precision_m: 0.6662 - val_recall_m: 0.4765 - val_nist: 0.7708\n",
      "\n",
      "Epoch 00009: val_f1_m improved from 0.51848 to 0.55391, saving model to training/BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-32-number-hidden-3-size-hidden-80-04_09_2019-02_04_26.h5\n",
      "Epoch 10/1000\n",
      "61237/61237 [==============================] - 1s 18us/step - loss: 0.0914 - acc: 0.9712 - f1_m: 0.4880 - precision_m: 0.6477 - recall_m: 0.3971 - nist: 0.8280 - val_loss: 0.1041 - val_acc: 0.9707 - val_f1_m: 0.5538 - val_precision_m: 0.6689 - val_recall_m: 0.4746 - val_nist: 0.7694\n",
      "\n",
      "Epoch 00010: val_f1_m did not improve from 0.55391\n",
      "Epoch 11/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0892 - acc: 0.9727 - f1_m: 0.5347 - precision_m: 0.6720 - recall_m: 0.4507 - nist: 0.7828 - val_loss: 0.1024 - val_acc: 0.9701 - val_f1_m: 0.5562 - val_precision_m: 0.6434 - val_recall_m: 0.4915 - val_nist: 0.7863\n",
      "\n",
      "Epoch 00011: val_f1_m improved from 0.55391 to 0.55625, saving model to training/BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-32-number-hidden-3-size-hidden-80-04_09_2019-02_04_26.h5\n",
      "Epoch 12/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0877 - acc: 0.9726 - f1_m: 0.5314 - precision_m: 0.6573 - recall_m: 0.4543 - nist: 0.8021 - val_loss: 0.1028 - val_acc: 0.9703 - val_f1_m: 0.5601 - val_precision_m: 0.6527 - val_recall_m: 0.4934 - val_nist: 0.7777\n",
      "\n",
      "Epoch 00012: val_f1_m improved from 0.55625 to 0.56009, saving model to training/BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-32-number-hidden-3-size-hidden-80-04_09_2019-02_04_26.h5\n",
      "Epoch 13/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0860 - acc: 0.9725 - f1_m: 0.5363 - precision_m: 0.6488 - recall_m: 0.4658 - nist: 0.7971 - val_loss: 0.1033 - val_acc: 0.9703 - val_f1_m: 0.5500 - val_precision_m: 0.6555 - val_recall_m: 0.4763 - val_nist: 0.7805\n",
      "\n",
      "Epoch 00013: val_f1_m did not improve from 0.56009\n",
      "Epoch 14/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0828 - acc: 0.9737 - f1_m: 0.5626 - precision_m: 0.6712 - recall_m: 0.4915 - nist: 0.7557 - val_loss: 0.1030 - val_acc: 0.9703 - val_f1_m: 0.5730 - val_precision_m: 0.6421 - val_recall_m: 0.5188 - val_nist: 0.7768\n",
      "\n",
      "Epoch 00014: val_f1_m improved from 0.56009 to 0.57299, saving model to training/BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-32-number-hidden-3-size-hidden-80-04_09_2019-02_04_26.h5\n",
      "Epoch 15/1000\n",
      "61237/61237 [==============================] - 1s 18us/step - loss: 0.0816 - acc: 0.9739 - f1_m: 0.5695 - precision_m: 0.6680 - recall_m: 0.5023 - nist: 0.7542 - val_loss: 0.1034 - val_acc: 0.9706 - val_f1_m: 0.5625 - val_precision_m: 0.6581 - val_recall_m: 0.4936 - val_nist: 0.7702\n",
      "\n",
      "Epoch 00015: val_f1_m did not improve from 0.57299\n",
      "Epoch 16/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0798 - acc: 0.9741 - f1_m: 0.5759 - precision_m: 0.6672 - recall_m: 0.5142 - nist: 0.7527 - val_loss: 0.1051 - val_acc: 0.9704 - val_f1_m: 0.5610 - val_precision_m: 0.6527 - val_recall_m: 0.4934 - val_nist: 0.7728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00016: val_f1_m did not improve from 0.57299\n",
      "Epoch 17/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0790 - acc: 0.9747 - f1_m: 0.5858 - precision_m: 0.6807 - recall_m: 0.5224 - nist: 0.7341 - val_loss: 0.1067 - val_acc: 0.9697 - val_f1_m: 0.5646 - val_precision_m: 0.6358 - val_recall_m: 0.5101 - val_nist: 0.7900\n",
      "\n",
      "Epoch 00017: val_f1_m did not improve from 0.57299\n",
      "Epoch 18/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0769 - acc: 0.9751 - f1_m: 0.5973 - precision_m: 0.6850 - recall_m: 0.5376 - nist: 0.7163 - val_loss: 0.1057 - val_acc: 0.9704 - val_f1_m: 0.5693 - val_precision_m: 0.6512 - val_recall_m: 0.5073 - val_nist: 0.7692\n",
      "\n",
      "Epoch 00018: val_f1_m did not improve from 0.57299\n",
      "Epoch 19/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0756 - acc: 0.9749 - f1_m: 0.5993 - precision_m: 0.6761 - recall_m: 0.5442 - nist: 0.7226 - val_loss: 0.1072 - val_acc: 0.9679 - val_f1_m: 0.5533 - val_precision_m: 0.5978 - val_recall_m: 0.5158 - val_nist: 0.8387\n",
      "\n",
      "Epoch 00019: val_f1_m did not improve from 0.57299\n",
      "Epoch 20/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0744 - acc: 0.9759 - f1_m: 0.6118 - precision_m: 0.6886 - recall_m: 0.5592 - nist: 0.7024 - val_loss: 0.1102 - val_acc: 0.9691 - val_f1_m: 0.5527 - val_precision_m: 0.6244 - val_recall_m: 0.4969 - val_nist: 0.8092\n",
      "\n",
      "Epoch 00020: val_f1_m did not improve from 0.57299\n",
      "Epoch 21/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0735 - acc: 0.9754 - f1_m: 0.6130 - precision_m: 0.6801 - recall_m: 0.5656 - nist: 0.7112 - val_loss: 0.1071 - val_acc: 0.9692 - val_f1_m: 0.5664 - val_precision_m: 0.6244 - val_recall_m: 0.5194 - val_nist: 0.8005\n",
      "\n",
      "Epoch 00021: val_f1_m did not improve from 0.57299\n",
      "Epoch 22/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0718 - acc: 0.9762 - f1_m: 0.6267 - precision_m: 0.6917 - recall_m: 0.5793 - nist: 0.6874 - val_loss: 0.1105 - val_acc: 0.9689 - val_f1_m: 0.5620 - val_precision_m: 0.6149 - val_recall_m: 0.5190 - val_nist: 0.8147\n",
      "\n",
      "Epoch 00022: val_f1_m did not improve from 0.57299\n",
      "Epoch 23/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0693 - acc: 0.9769 - f1_m: 0.6386 - precision_m: 0.7003 - recall_m: 0.5954 - nist: 0.6714 - val_loss: 0.1113 - val_acc: 0.9697 - val_f1_m: 0.5577 - val_precision_m: 0.6363 - val_recall_m: 0.4972 - val_nist: 0.7923\n",
      "\n",
      "Epoch 00023: val_f1_m did not improve from 0.57299\n",
      "Epoch 24/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0674 - acc: 0.9769 - f1_m: 0.6382 - precision_m: 0.7007 - recall_m: 0.5936 - nist: 0.6683 - val_loss: 0.1162 - val_acc: 0.9706 - val_f1_m: 0.5709 - val_precision_m: 0.6481 - val_recall_m: 0.5109 - val_nist: 0.7688\n",
      "\n",
      "Epoch 00024: val_f1_m did not improve from 0.57299\n",
      "Epoch 25/1000\n",
      "61237/61237 [==============================] - 1s 18us/step - loss: 0.0673 - acc: 0.9775 - f1_m: 0.6523 - precision_m: 0.7039 - recall_m: 0.6148 - nist: 0.6542 - val_loss: 0.1156 - val_acc: 0.9689 - val_f1_m: 0.5604 - val_precision_m: 0.6201 - val_recall_m: 0.5128 - val_nist: 0.8094\n",
      "\n",
      "Epoch 00025: val_f1_m did not improve from 0.57299\n",
      "Epoch 26/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0667 - acc: 0.9773 - f1_m: 0.6494 - precision_m: 0.7051 - recall_m: 0.6064 - nist: 0.6542 - val_loss: 0.1150 - val_acc: 0.9678 - val_f1_m: 0.5533 - val_precision_m: 0.6004 - val_recall_m: 0.5159 - val_nist: 0.8410\n",
      "\n",
      "Epoch 00026: val_f1_m did not improve from 0.57299\n",
      "Epoch 27/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0637 - acc: 0.9780 - f1_m: 0.6625 - precision_m: 0.7102 - recall_m: 0.6256 - nist: 0.6375 - val_loss: 0.1159 - val_acc: 0.9672 - val_f1_m: 0.5545 - val_precision_m: 0.5869 - val_recall_m: 0.5289 - val_nist: 0.8563\n",
      "\n",
      "Epoch 00027: val_f1_m did not improve from 0.57299\n",
      "Epoch 28/1000\n",
      "61237/61237 [==============================] - 1s 18us/step - loss: 0.0637 - acc: 0.9783 - f1_m: 0.6682 - precision_m: 0.7187 - recall_m: 0.6287 - nist: 0.6223 - val_loss: 0.1182 - val_acc: 0.9688 - val_f1_m: 0.5601 - val_precision_m: 0.6134 - val_recall_m: 0.5169 - val_nist: 0.8146\n",
      "\n",
      "Epoch 00028: val_f1_m did not improve from 0.57299\n",
      "Epoch 29/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0604 - acc: 0.9792 - f1_m: 0.6817 - precision_m: 0.7227 - recall_m: 0.6496 - nist: 0.6044 - val_loss: 0.1200 - val_acc: 0.9673 - val_f1_m: 0.5513 - val_precision_m: 0.5868 - val_recall_m: 0.5224 - val_nist: 0.8537\n",
      "\n",
      "Epoch 00029: val_f1_m did not improve from 0.57299\n",
      "Epoch 30/1000\n",
      "61237/61237 [==============================] - 1s 18us/step - loss: 0.0616 - acc: 0.9788 - f1_m: 0.6765 - precision_m: 0.7217 - recall_m: 0.6453 - nist: 0.6139 - val_loss: 0.1189 - val_acc: 0.9681 - val_f1_m: 0.5539 - val_precision_m: 0.6011 - val_recall_m: 0.5161 - val_nist: 0.8349\n",
      "\n",
      "Epoch 00030: val_f1_m did not improve from 0.57299\n",
      "Epoch 31/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0592 - acc: 0.9791 - f1_m: 0.6846 - precision_m: 0.7188 - recall_m: 0.6589 - nist: 0.6063 - val_loss: 0.1194 - val_acc: 0.9667 - val_f1_m: 0.5374 - val_precision_m: 0.5776 - val_recall_m: 0.5033 - val_nist: 0.8713\n",
      "\n",
      "Epoch 00031: val_f1_m did not improve from 0.57299\n",
      "Epoch 32/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0576 - acc: 0.9799 - f1_m: 0.6970 - precision_m: 0.7334 - recall_m: 0.6687 - nist: 0.5813 - val_loss: 0.1238 - val_acc: 0.9670 - val_f1_m: 0.5574 - val_precision_m: 0.5741 - val_recall_m: 0.5437 - val_nist: 0.8661\n",
      "\n",
      "Epoch 00032: val_f1_m did not improve from 0.57299\n",
      "Epoch 33/1000\n",
      "61237/61237 [==============================] - 1s 18us/step - loss: 0.0569 - acc: 0.9799 - f1_m: 0.6988 - precision_m: 0.7358 - recall_m: 0.6731 - nist: 0.5770 - val_loss: 0.1224 - val_acc: 0.9676 - val_f1_m: 0.5469 - val_precision_m: 0.5945 - val_recall_m: 0.5090 - val_nist: 0.8451\n",
      "\n",
      "Epoch 00033: val_f1_m did not improve from 0.57299\n",
      "Epoch 34/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0562 - acc: 0.9800 - f1_m: 0.7033 - precision_m: 0.7297 - recall_m: 0.6860 - nist: 0.5783 - val_loss: 0.1255 - val_acc: 0.9658 - val_f1_m: 0.5309 - val_precision_m: 0.5695 - val_recall_m: 0.5004 - val_nist: 0.8897\n",
      "\n",
      "Epoch 00034: val_f1_m did not improve from 0.57299\n",
      "Epoch 35/1000\n",
      "61237/61237 [==============================] - 1s 18us/step - loss: 0.0549 - acc: 0.9801 - f1_m: 0.6994 - precision_m: 0.7417 - recall_m: 0.6694 - nist: 0.5739 - val_loss: 0.1245 - val_acc: 0.9673 - val_f1_m: 0.5410 - val_precision_m: 0.5845 - val_recall_m: 0.5047 - val_nist: 0.8562\n",
      "\n",
      "Epoch 00035: val_f1_m did not improve from 0.57299\n",
      "Epoch 36/1000\n",
      "61237/61237 [==============================] - 1s 18us/step - loss: 0.0534 - acc: 0.9815 - f1_m: 0.7212 - precision_m: 0.7574 - recall_m: 0.6937 - nist: 0.5360 - val_loss: 0.1306 - val_acc: 0.9667 - val_f1_m: 0.5481 - val_precision_m: 0.5756 - val_recall_m: 0.5260 - val_nist: 0.8733\n",
      "\n",
      "Epoch 00036: val_f1_m did not improve from 0.57299\n",
      "Epoch 37/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0525 - acc: 0.9814 - f1_m: 0.7219 - precision_m: 0.7468 - recall_m: 0.7037 - nist: 0.5427 - val_loss: 0.1271 - val_acc: 0.9676 - val_f1_m: 0.5478 - val_precision_m: 0.5914 - val_recall_m: 0.5120 - val_nist: 0.8478\n",
      "\n",
      "Epoch 00037: val_f1_m did not improve from 0.57299\n",
      "Epoch 38/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0516 - acc: 0.9809 - f1_m: 0.7166 - precision_m: 0.7403 - recall_m: 0.7012 - nist: 0.5535 - val_loss: 0.1331 - val_acc: 0.9663 - val_f1_m: 0.5482 - val_precision_m: 0.5657 - val_recall_m: 0.5332 - val_nist: 0.8843\n",
      "\n",
      "Epoch 00038: val_f1_m did not improve from 0.57299\n",
      "Epoch 39/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0520 - acc: 0.9814 - f1_m: 0.7234 - precision_m: 0.7485 - recall_m: 0.7041 - nist: 0.5386 - val_loss: 0.1295 - val_acc: 0.9679 - val_f1_m: 0.5526 - val_precision_m: 0.5903 - val_recall_m: 0.5224 - val_nist: 0.8443\n",
      "\n",
      "Epoch 00039: val_f1_m did not improve from 0.57299\n",
      "Epoch 40/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0485 - acc: 0.9827 - f1_m: 0.7434 - precision_m: 0.7676 - recall_m: 0.7274 - nist: 0.5049 - val_loss: 0.1311 - val_acc: 0.9669 - val_f1_m: 0.5467 - val_precision_m: 0.5749 - val_recall_m: 0.5239 - val_nist: 0.8672\n",
      "\n",
      "Epoch 00040: val_f1_m did not improve from 0.57299\n",
      "Epoch 41/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0491 - acc: 0.9821 - f1_m: 0.7312 - precision_m: 0.7622 - recall_m: 0.7088 - nist: 0.5196 - val_loss: 0.1288 - val_acc: 0.9650 - val_f1_m: 0.5428 - val_precision_m: 0.5495 - val_recall_m: 0.5410 - val_nist: 0.9153\n",
      "\n",
      "Epoch 00041: val_f1_m did not improve from 0.57299\n",
      "Epoch 42/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0479 - acc: 0.9825 - f1_m: 0.7378 - precision_m: 0.7650 - recall_m: 0.7175 - nist: 0.5066 - val_loss: 0.1302 - val_acc: 0.9642 - val_f1_m: 0.5230 - val_precision_m: 0.5334 - val_recall_m: 0.5143 - val_nist: 0.9375\n",
      "\n",
      "Epoch 00042: val_f1_m did not improve from 0.57299\n",
      "Epoch 43/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0481 - acc: 0.9824 - f1_m: 0.7345 - precision_m: 0.7758 - recall_m: 0.7046 - nist: 0.5062 - val_loss: 0.1352 - val_acc: 0.9660 - val_f1_m: 0.5217 - val_precision_m: 0.5663 - val_recall_m: 0.4865 - val_nist: 0.8873\n",
      "\n",
      "Epoch 00043: val_f1_m did not improve from 0.57299\n",
      "Epoch 44/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0470 - acc: 0.9829 - f1_m: 0.7431 - precision_m: 0.7742 - recall_m: 0.7199 - nist: 0.4960 - val_loss: 0.1370 - val_acc: 0.9663 - val_f1_m: 0.5320 - val_precision_m: 0.5688 - val_recall_m: 0.5033 - val_nist: 0.8846\n",
      "\n",
      "Epoch 00044: val_f1_m did not improve from 0.57299\n",
      "Epoch 45/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0452 - acc: 0.9833 - f1_m: 0.7508 - precision_m: 0.7761 - recall_m: 0.7338 - nist: 0.4826 - val_loss: 0.1384 - val_acc: 0.9645 - val_f1_m: 0.5331 - val_precision_m: 0.5400 - val_recall_m: 0.5285 - val_nist: 0.9282\n",
      "\n",
      "Epoch 00045: val_f1_m did not improve from 0.57299\n",
      "Epoch 46/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0456 - acc: 0.9837 - f1_m: 0.7569 - precision_m: 0.7804 - recall_m: 0.7402 - nist: 0.4739 - val_loss: 0.1417 - val_acc: 0.9638 - val_f1_m: 0.5277 - val_precision_m: 0.5326 - val_recall_m: 0.5239 - val_nist: 0.9440\n",
      "\n",
      "Epoch 00046: val_f1_m did not improve from 0.57299\n",
      "Epoch 47/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0462 - acc: 0.9826 - f1_m: 0.7404 - precision_m: 0.7709 - recall_m: 0.7207 - nist: 0.5003 - val_loss: 0.1382 - val_acc: 0.9630 - val_f1_m: 0.4705 - val_precision_m: 0.5207 - val_recall_m: 0.4314 - val_nist: 0.9657\n",
      "\n",
      "Epoch 00047: val_f1_m did not improve from 0.57299\n",
      "Epoch 48/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0447 - acc: 0.9836 - f1_m: 0.7528 - precision_m: 0.7893 - recall_m: 0.7248 - nist: 0.4737 - val_loss: 0.1385 - val_acc: 0.9655 - val_f1_m: 0.5254 - val_precision_m: 0.5610 - val_recall_m: 0.4955 - val_nist: 0.8964\n",
      "\n",
      "Epoch 00048: val_f1_m did not improve from 0.57299\n",
      "Epoch 49/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0429 - acc: 0.9842 - f1_m: 0.7612 - precision_m: 0.7995 - recall_m: 0.7318 - nist: 0.4561 - val_loss: 0.1484 - val_acc: 0.9629 - val_f1_m: 0.5081 - val_precision_m: 0.5254 - val_recall_m: 0.4941 - val_nist: 0.9650\n",
      "\n",
      "Epoch 00049: val_f1_m did not improve from 0.57299\n",
      "Epoch 50/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0426 - acc: 0.9841 - f1_m: 0.7594 - precision_m: 0.7980 - recall_m: 0.7294 - nist: 0.4607 - val_loss: 0.1470 - val_acc: 0.9651 - val_f1_m: 0.5101 - val_precision_m: 0.5517 - val_recall_m: 0.4768 - val_nist: 0.9055\n",
      "\n",
      "Epoch 00050: val_f1_m did not improve from 0.57299\n",
      "Epoch 51/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0419 - acc: 0.9849 - f1_m: 0.7732 - precision_m: 0.8049 - recall_m: 0.7477 - nist: 0.4388 - val_loss: 0.1484 - val_acc: 0.9657 - val_f1_m: 0.5115 - val_precision_m: 0.5631 - val_recall_m: 0.4700 - val_nist: 0.8959\n",
      "\n",
      "Epoch 00051: val_f1_m did not improve from 0.57299\n",
      "Epoch 52/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0409 - acc: 0.9853 - f1_m: 0.7751 - precision_m: 0.8151 - recall_m: 0.7437 - nist: 0.4279 - val_loss: 0.1458 - val_acc: 0.9664 - val_f1_m: 0.5115 - val_precision_m: 0.5762 - val_recall_m: 0.4639 - val_nist: 0.8771\n",
      "\n",
      "Epoch 00052: val_f1_m did not improve from 0.57299\n",
      "Epoch 53/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0408 - acc: 0.9850 - f1_m: 0.7746 - precision_m: 0.8063 - recall_m: 0.7509 - nist: 0.4365 - val_loss: 0.1478 - val_acc: 0.9636 - val_f1_m: 0.4906 - val_precision_m: 0.5310 - val_recall_m: 0.4588 - val_nist: 0.9478\n",
      "\n",
      "Epoch 00053: val_f1_m did not improve from 0.57299\n",
      "Epoch 54/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0380 - acc: 0.9862 - f1_m: 0.7918 - precision_m: 0.8305 - recall_m: 0.7627 - nist: 0.3994 - val_loss: 0.1503 - val_acc: 0.9648 - val_f1_m: 0.4958 - val_precision_m: 0.5531 - val_recall_m: 0.4519 - val_nist: 0.9179\n",
      "\n",
      "Epoch 00054: val_f1_m did not improve from 0.57299\n",
      "Epoch 55/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0400 - acc: 0.9856 - f1_m: 0.7814 - precision_m: 0.8247 - recall_m: 0.7508 - nist: 0.4195 - val_loss: 0.1463 - val_acc: 0.9636 - val_f1_m: 0.4842 - val_precision_m: 0.5329 - val_recall_m: 0.4452 - val_nist: 0.9462\n",
      "\n",
      "Epoch 00055: val_f1_m did not improve from 0.57299\n",
      "Epoch 56/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0403 - acc: 0.9851 - f1_m: 0.7746 - precision_m: 0.8169 - recall_m: 0.7406 - nist: 0.4305 - val_loss: 0.1510 - val_acc: 0.9644 - val_f1_m: 0.4944 - val_precision_m: 0.5393 - val_recall_m: 0.4603 - val_nist: 0.9349\n",
      "\n",
      "Epoch 00056: val_f1_m did not improve from 0.57299\n",
      "Epoch 57/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0362 - acc: 0.9862 - f1_m: 0.7951 - precision_m: 0.8205 - recall_m: 0.7762 - nist: 0.4003 - val_loss: 0.1578 - val_acc: 0.9653 - val_f1_m: 0.5159 - val_precision_m: 0.5567 - val_recall_m: 0.4843 - val_nist: 0.9059\n",
      "\n",
      "Epoch 00057: val_f1_m did not improve from 0.57299\n",
      "Epoch 58/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0373 - acc: 0.9867 - f1_m: 0.8017 - precision_m: 0.8254 - recall_m: 0.7844 - nist: 0.3883 - val_loss: 0.1560 - val_acc: 0.9663 - val_f1_m: 0.5168 - val_precision_m: 0.5740 - val_recall_m: 0.4752 - val_nist: 0.8778\n",
      "\n",
      "Epoch 00058: val_f1_m did not improve from 0.57299\n",
      "Epoch 59/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0362 - acc: 0.9868 - f1_m: 0.7989 - precision_m: 0.8396 - recall_m: 0.7666 - nist: 0.3821 - val_loss: 0.1522 - val_acc: 0.9647 - val_f1_m: 0.5143 - val_precision_m: 0.5419 - val_recall_m: 0.4900 - val_nist: 0.9236\n",
      "\n",
      "Epoch 00059: val_f1_m did not improve from 0.57299\n",
      "Epoch 60/1000\n",
      "61237/61237 [==============================] - 1s 18us/step - loss: 0.0358 - acc: 0.9871 - f1_m: 0.8073 - precision_m: 0.8498 - recall_m: 0.7744 - nist: 0.3664 - val_loss: 0.1526 - val_acc: 0.9641 - val_f1_m: 0.5083 - val_precision_m: 0.5351 - val_recall_m: 0.4871 - val_nist: 0.9400\n",
      "\n",
      "Epoch 00060: val_f1_m did not improve from 0.57299\n",
      "Epoch 61/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0374 - acc: 0.9864 - f1_m: 0.7922 - precision_m: 0.8416 - recall_m: 0.7554 - nist: 0.3942 - val_loss: 0.1596 - val_acc: 0.9644 - val_f1_m: 0.5037 - val_precision_m: 0.5415 - val_recall_m: 0.4737 - val_nist: 0.9357\n",
      "\n",
      "Epoch 00061: val_f1_m did not improve from 0.57299\n",
      "Epoch 62/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0360 - acc: 0.9870 - f1_m: 0.8027 - precision_m: 0.8464 - recall_m: 0.7668 - nist: 0.3761 - val_loss: 0.1583 - val_acc: 0.9644 - val_f1_m: 0.5063 - val_precision_m: 0.5397 - val_recall_m: 0.4788 - val_nist: 0.9360\n",
      "\n",
      "Epoch 00062: val_f1_m did not improve from 0.57299\n",
      "Epoch 63/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0351 - acc: 0.9870 - f1_m: 0.8053 - precision_m: 0.8375 - recall_m: 0.7819 - nist: 0.3751 - val_loss: 0.1615 - val_acc: 0.9641 - val_f1_m: 0.4923 - val_precision_m: 0.5371 - val_recall_m: 0.4597 - val_nist: 0.9388\n",
      "\n",
      "Epoch 00063: val_f1_m did not improve from 0.57299\n",
      "Epoch 64/1000\n",
      "61237/61237 [==============================] - 1s 18us/step - loss: 0.0337 - acc: 0.9870 - f1_m: 0.8048 - precision_m: 0.8326 - recall_m: 0.7831 - nist: 0.3774 - val_loss: 0.1628 - val_acc: 0.9630 - val_f1_m: 0.4837 - val_precision_m: 0.5223 - val_recall_m: 0.4554 - val_nist: 0.9697\n",
      "\n",
      "Epoch 00064: val_f1_m did not improve from 0.57299\n",
      "Epoch 65/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0335 - acc: 0.9881 - f1_m: 0.8190 - precision_m: 0.8625 - recall_m: 0.7845 - nist: 0.3449 - val_loss: 0.1638 - val_acc: 0.9617 - val_f1_m: 0.4951 - val_precision_m: 0.5138 - val_recall_m: 0.4823 - val_nist: 0.9942\n",
      "\n",
      "Epoch 00065: val_f1_m did not improve from 0.57299\n",
      "Epoch 66/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0314 - acc: 0.9884 - f1_m: 0.8256 - precision_m: 0.8628 - recall_m: 0.7950 - nist: 0.3341 - val_loss: 0.1740 - val_acc: 0.9630 - val_f1_m: 0.4745 - val_precision_m: 0.5197 - val_recall_m: 0.4391 - val_nist: 0.9679\n",
      "\n",
      "Epoch 00066: val_f1_m did not improve from 0.57299\n",
      "Epoch 67/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0322 - acc: 0.9878 - f1_m: 0.8151 - precision_m: 0.8540 - recall_m: 0.7853 - nist: 0.3538 - val_loss: 0.1730 - val_acc: 0.9650 - val_f1_m: 0.4799 - val_precision_m: 0.5562 - val_recall_m: 0.4241 - val_nist: 0.9149\n",
      "\n",
      "Epoch 00067: val_f1_m did not improve from 0.57299\n",
      "Epoch 68/1000\n",
      "61237/61237 [==============================] - 1s 19us/step - loss: 0.0327 - acc: 0.9883 - f1_m: 0.8241 - precision_m: 0.8681 - recall_m: 0.7888 - nist: 0.3357 - val_loss: 0.1668 - val_acc: 0.9638 - val_f1_m: 0.5107 - val_precision_m: 0.5293 - val_recall_m: 0.4972 - val_nist: 0.9472\n",
      "\n",
      "Epoch 00068: val_f1_m did not improve from 0.57299\n",
      "Epoch 69/1000\n",
      "61237/61237 [==============================] - 1s 20us/step - loss: 0.0317 - acc: 0.9884 - f1_m: 0.8261 - precision_m: 0.8580 - recall_m: 0.8011 - nist: 0.3349 - val_loss: 0.1779 - val_acc: 0.9623 - val_f1_m: 0.4633 - val_precision_m: 0.5165 - val_recall_m: 0.4240 - val_nist: 0.9849\n",
      "\n",
      "Epoch 00069: val_f1_m did not improve from 0.57299\n",
      "Epoch 70/1000\n",
      "11000/61237 [====>.........................] - ETA: 0s - loss: 0.0287 - acc: 0.9886 - f1_m: 0.8322 - precision_m: 0.8773 - recall_m: 0.7938 - nist: 0.3208"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-bc11db703fce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m#             early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6, min_delta=0.001)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/local/scratch/mac224/anaconda/envs/SUNDER/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/local/scratch/mac224/anaconda/envs/SUNDER/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/mac224/anaconda/envs/SUNDER/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/mac224/anaconda/envs/SUNDER/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/mac224/anaconda/envs/SUNDER/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "numbers_hidden = [3]\n",
    "sizes_hidden = [80]\n",
    "LSTM_sizes = [32]\n",
    "\n",
    "weights = class_weight.compute_class_weight('balanced',\n",
    "                                            np.unique(y_train),\n",
    "                                            np.array(y_train).reshape(-1))\n",
    "weigths = {index: value for index, value in enumerate(weights)}\n",
    "for LSTM_size in LSTM_sizes:\n",
    "    for size in sizes_hidden:\n",
    "        for number in numbers_hidden:\n",
    "            model = LSTM_model(LSTM_size, number, size)\n",
    "\n",
    "            name = f'BULATS-true-y-smaller-window-LSTM-without-speaker-turn-pause-true-next-word-input-dropout-automatic-input-weights-automatic-split-LSTM-size-{LSTM_size}-number-hidden-{number}-size-hidden-{size}-{datetime.now().strftime(\"%d_%m_%Y-%H_%M_%S\")}'\n",
    "            checkpoint = ModelCheckpoint(f'training/{name}.h5', monitor='val_f1_m', verbose=1, save_best_only=True, mode='max')\n",
    "            tensorboard = TensorBoard(log_dir=f\"training/tensorboard/{name}\")\n",
    "#             early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6, min_delta=0.001)\n",
    "\n",
    "            model.fit(x_tr, y_train, epochs=1000, batch_size=1000, callbacks=[checkpoint, tensorboard], validation_data=(x_ts, y_test), shuffle=True, class_weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to confirm that our calculation of f-scores, recall and precision are correct we load one of the models and predict the test set on it. An important thing to notice is the loading of custom_objects. Those are required functions as those were used in trainig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      6531\n",
      "           1       0.70      0.72      0.71       261\n",
      "\n",
      "    accuracy                           0.98      6792\n",
      "   macro avg       0.84      0.85      0.85      6792\n",
      "weighted avg       0.98      0.98      0.98      6792\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"training/BULATS-old-features-bigger-network-automatic-weights-04_09_2019-02_30_55.h5\", custom_objects={'f1_m': f1_m, 'precision_m': precision_m, 'recall_m': recall_m, 'nist': nist})\n",
    "result = model.predict(x_ts)\n",
    "test['break'] = np.round(result, 3)\n",
    "print(classification_report(y_test, np.around(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some statistical information about the data. It helps to consider whether the dataset is somehow biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pauses (1348, 4)\n",
      "Number of pauses when we think there is a break (1139, 4)\n",
      "Number of pauses when we think there is a break but there is not (403, 4)\n",
      "Number of pauses where there is no boundary (494, 4)\n",
      "Number of pauses where there is a boundary (854, 4)\n",
      "False negatives (833, 4)\n",
      "False positives (607, 4)\n",
      "True positives (1305, 4)\n",
      "True negatives (58490, 4)\n",
      "Total number of breaks (2139, 4)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 7000)\n",
    "\n",
    "print('Number of pauses', train.loc[train['postPause'] > 0, ['token', 'postPause', 'boundary', 'break']].shape)\n",
    "print('Number of pauses when we think there is a break', train.loc[(train['postPause'] > 0) & (train['break'] > 0.5), ['token', 'postPause', 'boundary', 'break']].shape)\n",
    "\n",
    "print('Number of pauses when we think there is a break but there is not', train.loc[(train['postPause'] > 0) & (train['boundary'] == False) & (train['break'] > 0.5), ['token', 'postPause', 'boundary', 'break']].shape)\n",
    "\n",
    "print('Number of pauses where there is no boundary', train.loc[(train['postPause'] > 0) & (train['boundary'] == False), ['token', 'postPause', 'boundary', 'break']].shape)\n",
    "print('Number of pauses where there is a boundary', train.loc[(train['postPause'] > 0) & (train['boundary'] == True), ['token', 'postPause', 'boundary', 'break']].shape)\n",
    "\n",
    "print('False negatives', train.loc[(train['break'] < 0.5) & (train['boundary'] == True), ['token', 'postPause', 'boundary', 'break']].shape)\n",
    "print('False positives', train.loc[(train['break'] > 0.5) & (train['boundary'] == False), ['token', 'postPause', 'boundary', 'break']].shape)\n",
    "print('True positives', train.loc[(train['break'] > 0.5) & (train['boundary'] == True), ['token', 'postPause', 'boundary', 'break']].shape)\n",
    "print('True negatives', train.loc[(train['break'] < 0.5) & (train['boundary'] == False), ['token', 'postPause', 'boundary', 'break']].shape)\n",
    "\n",
    "print('Total number of breaks', train.loc[train['boundary'] == True, ['token', 'postPause', 'boundary', 'break']].shape)\n",
    "\n",
    "\n",
    "train.loc[:, ['recording', 'token', 'postPause', 'boundary', 'break']].to_csv('new_train_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the influence of the varying acceptance threshold on the f-score results. In general those do not have a huge influence on the results but allow us to see how certain the network is. We would prefer to see a step as that would be the ideal shape - no results between 0 and 1 and all at those boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/scratch/mac224/anaconda/envs/SUNDER/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6648660814764799\n",
      "0.87209225\n"
     ]
    }
   ],
   "source": [
    "points = []\n",
    "for i in np.arange(0, 1, 0.01):\n",
    "    points.append(precision_recall_fscore_support(y_train, np.greater(result, i))[2][1])\n",
    "\n",
    "plt.plot(np.arange(0, 1, 0.01), points)\n",
    "plt.ylabel('F-score')\n",
    "plt.xlabel('threshold')\n",
    "plt.show()\n",
    "print(np.max(points))\n",
    "print(np.max(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we also possess the old fetaures which have been used in the previous research. By direct usage of those we can clearly compare our performance against the performance of the previous system. Thus, we load the necessary features and scale them in order to use in the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = train.loc[:, 'prePause':'EminWplus1'].values\n",
    "y_tr = train['boundary']\n",
    "\n",
    "x_ts = test.loc[:, 'prePause':'EminWplus1'].values\n",
    "y_ts = test['boundary']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_tr = scaler.fit_transform(x_tr)\n",
    "x_ts = scaler.transform(x_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a very simple fully connected model that will that those features in and predict the break. Once, again we use adam and binary crossentropy. For activation of the layers we use relu and after some intial test notice that overfitting is not a big problem, thus, do not include mechanisms such as the dropout or batch normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FC_model():\n",
    "    features_input = Input(shape=((23,)))    \n",
    "    x = Dense(40, activation='relu')(features_input)    \n",
    "    x = Dense(40, activation='relu')(x)    \n",
    "    x = Dense(40, activation='relu')(x)    \n",
    "    x = Dense(40, activation='relu')(x)    \n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=features_input, outputs=x)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy', f1_m, precision_m, recall_m, nist])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of the model is fast enough that we can do it live. We define checkpoints to save the best model and use tensorboard for data visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 61237 samples, validate on 6792 samples\n",
      "Epoch 1/1000\n",
      "61237/61237 [==============================] - 2s 30us/step - loss: 0.2323 - acc: 0.9658 - f1_m: 0.1528 - precision_m: 0.2399 - recall_m: 0.1236 - nist: 0.9800 - val_loss: 0.1261 - val_acc: 0.9678 - val_f1_m: 0.4409 - val_precision_m: 0.6859 - val_recall_m: 0.3289 - val_nist: 0.8279\n",
      "\n",
      "Epoch 00001: val_f1_m improved from -inf to 0.44090, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 2/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.1035 - acc: 0.9714 - f1_m: 0.4846 - precision_m: 0.6474 - recall_m: 0.3936 - nist: 0.8260 - val_loss: 0.1006 - val_acc: 0.9682 - val_f1_m: 0.4746 - val_precision_m: 0.6740 - val_recall_m: 0.3688 - val_nist: 0.8111\n",
      "\n",
      "Epoch 00002: val_f1_m improved from 0.44090 to 0.47460, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 3/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0875 - acc: 0.9733 - f1_m: 0.5594 - precision_m: 0.6563 - recall_m: 0.4969 - nist: 0.7743 - val_loss: 0.0878 - val_acc: 0.9719 - val_f1_m: 0.5932 - val_precision_m: 0.6908 - val_recall_m: 0.5240 - val_nist: 0.7196\n",
      "\n",
      "Epoch 00003: val_f1_m improved from 0.47460 to 0.59323, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 4/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0804 - acc: 0.9743 - f1_m: 0.6093 - precision_m: 0.6443 - recall_m: 0.5867 - nist: 0.7509 - val_loss: 0.0835 - val_acc: 0.9731 - val_f1_m: 0.6293 - val_precision_m: 0.6878 - val_recall_m: 0.5836 - val_nist: 0.6906\n",
      "\n",
      "Epoch 00004: val_f1_m improved from 0.59323 to 0.62935, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 5/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0767 - acc: 0.9747 - f1_m: 0.6274 - precision_m: 0.6395 - recall_m: 0.6231 - nist: 0.7394 - val_loss: 0.0815 - val_acc: 0.9745 - val_f1_m: 0.6587 - val_precision_m: 0.6884 - val_recall_m: 0.6340 - val_nist: 0.6607\n",
      "\n",
      "Epoch 00005: val_f1_m improved from 0.62935 to 0.65867, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 6/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0745 - acc: 0.9750 - f1_m: 0.6433 - precision_m: 0.6416 - recall_m: 0.6522 - nist: 0.7265 - val_loss: 0.0803 - val_acc: 0.9745 - val_f1_m: 0.6638 - val_precision_m: 0.6816 - val_recall_m: 0.6497 - val_nist: 0.6609\n",
      "\n",
      "Epoch 00006: val_f1_m improved from 0.65867 to 0.66385, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 7/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0731 - acc: 0.9756 - f1_m: 0.6560 - precision_m: 0.6440 - recall_m: 0.6741 - nist: 0.7047 - val_loss: 0.0792 - val_acc: 0.9750 - val_f1_m: 0.6754 - val_precision_m: 0.6838 - val_recall_m: 0.6697 - val_nist: 0.6477\n",
      "\n",
      "Epoch 00007: val_f1_m improved from 0.66385 to 0.67540, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 8/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0717 - acc: 0.9757 - f1_m: 0.6573 - precision_m: 0.6451 - recall_m: 0.6800 - nist: 0.7169 - val_loss: 0.0795 - val_acc: 0.9747 - val_f1_m: 0.6755 - val_precision_m: 0.6767 - val_recall_m: 0.6766 - val_nist: 0.6551\n",
      "\n",
      "Epoch 00008: val_f1_m improved from 0.67540 to 0.67549, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 9/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0706 - acc: 0.9759 - f1_m: 0.6634 - precision_m: 0.6437 - recall_m: 0.6903 - nist: 0.7032 - val_loss: 0.0769 - val_acc: 0.9753 - val_f1_m: 0.6883 - val_precision_m: 0.6786 - val_recall_m: 0.6998 - val_nist: 0.6387\n",
      "\n",
      "Epoch 00009: val_f1_m improved from 0.67549 to 0.68826, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 10/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0695 - acc: 0.9757 - f1_m: 0.6664 - precision_m: 0.6417 - recall_m: 0.7020 - nist: 0.7065 - val_loss: 0.0773 - val_acc: 0.9753 - val_f1_m: 0.6892 - val_precision_m: 0.6774 - val_recall_m: 0.7035 - val_nist: 0.6397\n",
      "\n",
      "Epoch 00010: val_f1_m improved from 0.68826 to 0.68920, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 11/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0688 - acc: 0.9760 - f1_m: 0.6657 - precision_m: 0.6449 - recall_m: 0.6947 - nist: 0.7033 - val_loss: 0.0758 - val_acc: 0.9759 - val_f1_m: 0.6988 - val_precision_m: 0.6827 - val_recall_m: 0.7171 - val_nist: 0.6227\n",
      "\n",
      "Epoch 00011: val_f1_m improved from 0.68920 to 0.69875, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 12/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0683 - acc: 0.9761 - f1_m: 0.6708 - precision_m: 0.6437 - recall_m: 0.7072 - nist: 0.6955 - val_loss: 0.0761 - val_acc: 0.9751 - val_f1_m: 0.6848 - val_precision_m: 0.6777 - val_recall_m: 0.6945 - val_nist: 0.6455\n",
      "\n",
      "Epoch 00012: val_f1_m did not improve from 0.69875\n",
      "Epoch 13/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0677 - acc: 0.9762 - f1_m: 0.6741 - precision_m: 0.6506 - recall_m: 0.7073 - nist: 0.6879 - val_loss: 0.0769 - val_acc: 0.9759 - val_f1_m: 0.6967 - val_precision_m: 0.6858 - val_recall_m: 0.7097 - val_nist: 0.6232\n",
      "\n",
      "Epoch 00013: val_f1_m did not improve from 0.69875\n",
      "Epoch 14/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0674 - acc: 0.9760 - f1_m: 0.6622 - precision_m: 0.6435 - recall_m: 0.6895 - nist: 0.7037 - val_loss: 0.0782 - val_acc: 0.9757 - val_f1_m: 0.6905 - val_precision_m: 0.6900 - val_recall_m: 0.6945 - val_nist: 0.6299\n",
      "\n",
      "Epoch 00014: val_f1_m did not improve from 0.69875\n",
      "Epoch 15/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0664 - acc: 0.9764 - f1_m: 0.6722 - precision_m: 0.6472 - recall_m: 0.7043 - nist: 0.6914 - val_loss: 0.0778 - val_acc: 0.9763 - val_f1_m: 0.7006 - val_precision_m: 0.6974 - val_recall_m: 0.7063 - val_nist: 0.6096\n",
      "\n",
      "Epoch 00015: val_f1_m improved from 0.69875 to 0.70063, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 16/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0657 - acc: 0.9762 - f1_m: 0.6686 - precision_m: 0.6482 - recall_m: 0.6996 - nist: 0.6986 - val_loss: 0.0751 - val_acc: 0.9751 - val_f1_m: 0.6932 - val_precision_m: 0.6724 - val_recall_m: 0.7171 - val_nist: 0.6412\n",
      "\n",
      "Epoch 00016: val_f1_m did not improve from 0.70063\n",
      "Epoch 17/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0652 - acc: 0.9766 - f1_m: 0.6739 - precision_m: 0.6546 - recall_m: 0.7018 - nist: 0.6808 - val_loss: 0.0761 - val_acc: 0.9759 - val_f1_m: 0.7004 - val_precision_m: 0.6828 - val_recall_m: 0.7205 - val_nist: 0.6225\n",
      "\n",
      "Epoch 00017: val_f1_m did not improve from 0.70063\n",
      "Epoch 18/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0652 - acc: 0.9764 - f1_m: 0.6723 - precision_m: 0.6510 - recall_m: 0.7029 - nist: 0.6901 - val_loss: 0.0744 - val_acc: 0.9756 - val_f1_m: 0.6959 - val_precision_m: 0.6793 - val_recall_m: 0.7154 - val_nist: 0.6325\n",
      "\n",
      "Epoch 00018: val_f1_m did not improve from 0.70063\n",
      "Epoch 19/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0643 - acc: 0.9766 - f1_m: 0.6757 - precision_m: 0.6488 - recall_m: 0.7126 - nist: 0.6863 - val_loss: 0.0742 - val_acc: 0.9763 - val_f1_m: 0.7054 - val_precision_m: 0.6931 - val_recall_m: 0.7205 - val_nist: 0.6103\n",
      "\n",
      "Epoch 00019: val_f1_m improved from 0.70063 to 0.70539, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 20/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0636 - acc: 0.9768 - f1_m: 0.6734 - precision_m: 0.6546 - recall_m: 0.7003 - nist: 0.6812 - val_loss: 0.0744 - val_acc: 0.9764 - val_f1_m: 0.7065 - val_precision_m: 0.6964 - val_recall_m: 0.7209 - val_nist: 0.6095\n",
      "\n",
      "Epoch 00020: val_f1_m improved from 0.70539 to 0.70651, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 21/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0634 - acc: 0.9766 - f1_m: 0.6777 - precision_m: 0.6536 - recall_m: 0.7113 - nist: 0.6824 - val_loss: 0.0757 - val_acc: 0.9756 - val_f1_m: 0.6967 - val_precision_m: 0.6846 - val_recall_m: 0.7124 - val_nist: 0.6290\n",
      "\n",
      "Epoch 00021: val_f1_m did not improve from 0.70651\n",
      "Epoch 22/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0630 - acc: 0.9772 - f1_m: 0.6809 - precision_m: 0.6598 - recall_m: 0.7137 - nist: 0.6686 - val_loss: 0.0741 - val_acc: 0.9760 - val_f1_m: 0.7054 - val_precision_m: 0.6871 - val_recall_m: 0.7273 - val_nist: 0.6172\n",
      "\n",
      "Epoch 00022: val_f1_m did not improve from 0.70651\n",
      "Epoch 23/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0622 - acc: 0.9769 - f1_m: 0.6795 - precision_m: 0.6628 - recall_m: 0.7056 - nist: 0.6655 - val_loss: 0.0740 - val_acc: 0.9760 - val_f1_m: 0.7024 - val_precision_m: 0.6883 - val_recall_m: 0.7209 - val_nist: 0.6203\n",
      "\n",
      "Epoch 00023: val_f1_m did not improve from 0.70651\n",
      "Epoch 24/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0621 - acc: 0.9771 - f1_m: 0.6805 - precision_m: 0.6606 - recall_m: 0.7091 - nist: 0.6662 - val_loss: 0.0743 - val_acc: 0.9763 - val_f1_m: 0.7091 - val_precision_m: 0.6907 - val_recall_m: 0.7310 - val_nist: 0.6099\n",
      "\n",
      "Epoch 00024: val_f1_m improved from 0.70651 to 0.70906, saving model to training/BULATS-old-features-deeper-bigger-network-automatic-weights-04_09_2019-02_32_30.h5\n",
      "Epoch 25/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0617 - acc: 0.9772 - f1_m: 0.6846 - precision_m: 0.6616 - recall_m: 0.7166 - nist: 0.6600 - val_loss: 0.0748 - val_acc: 0.9759 - val_f1_m: 0.6953 - val_precision_m: 0.6921 - val_recall_m: 0.7025 - val_nist: 0.6234\n",
      "\n",
      "Epoch 00025: val_f1_m did not improve from 0.70906\n",
      "Epoch 26/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0616 - acc: 0.9771 - f1_m: 0.6787 - precision_m: 0.6633 - recall_m: 0.7043 - nist: 0.6714 - val_loss: 0.0745 - val_acc: 0.9754 - val_f1_m: 0.6926 - val_precision_m: 0.6839 - val_recall_m: 0.7075 - val_nist: 0.6371\n",
      "\n",
      "Epoch 00026: val_f1_m did not improve from 0.70906\n",
      "Epoch 27/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0607 - acc: 0.9773 - f1_m: 0.6826 - precision_m: 0.6626 - recall_m: 0.7112 - nist: 0.6620 - val_loss: 0.0738 - val_acc: 0.9763 - val_f1_m: 0.7088 - val_precision_m: 0.6892 - val_recall_m: 0.7329 - val_nist: 0.6117\n",
      "\n",
      "Epoch 00027: val_f1_m did not improve from 0.70906\n",
      "Epoch 28/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0604 - acc: 0.9773 - f1_m: 0.6852 - precision_m: 0.6643 - recall_m: 0.7157 - nist: 0.6567 - val_loss: 0.0743 - val_acc: 0.9760 - val_f1_m: 0.7066 - val_precision_m: 0.6857 - val_recall_m: 0.7326 - val_nist: 0.6186\n",
      "\n",
      "Epoch 00028: val_f1_m did not improve from 0.70906\n",
      "Epoch 29/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0596 - acc: 0.9774 - f1_m: 0.6867 - precision_m: 0.6629 - recall_m: 0.7189 - nist: 0.6588 - val_loss: 0.0735 - val_acc: 0.9754 - val_f1_m: 0.6928 - val_precision_m: 0.6799 - val_recall_m: 0.7107 - val_nist: 0.6373\n",
      "\n",
      "Epoch 00029: val_f1_m did not improve from 0.70906\n",
      "Epoch 30/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0595 - acc: 0.9773 - f1_m: 0.6780 - precision_m: 0.6620 - recall_m: 0.7018 - nist: 0.6615 - val_loss: 0.0753 - val_acc: 0.9757 - val_f1_m: 0.7024 - val_precision_m: 0.6793 - val_recall_m: 0.7310 - val_nist: 0.6290\n",
      "\n",
      "Epoch 00030: val_f1_m did not improve from 0.70906\n",
      "Epoch 31/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0592 - acc: 0.9773 - f1_m: 0.6829 - precision_m: 0.6623 - recall_m: 0.7117 - nist: 0.6611 - val_loss: 0.0741 - val_acc: 0.9759 - val_f1_m: 0.7033 - val_precision_m: 0.6815 - val_recall_m: 0.7292 - val_nist: 0.6239\n",
      "\n",
      "Epoch 00031: val_f1_m did not improve from 0.70906\n",
      "Epoch 32/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0587 - acc: 0.9774 - f1_m: 0.6851 - precision_m: 0.6635 - recall_m: 0.7162 - nist: 0.6623 - val_loss: 0.0745 - val_acc: 0.9753 - val_f1_m: 0.6939 - val_precision_m: 0.6770 - val_recall_m: 0.7158 - val_nist: 0.6407\n",
      "\n",
      "Epoch 00032: val_f1_m did not improve from 0.70906\n",
      "Epoch 33/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0584 - acc: 0.9777 - f1_m: 0.6859 - precision_m: 0.6738 - recall_m: 0.7055 - nist: 0.6467 - val_loss: 0.0744 - val_acc: 0.9748 - val_f1_m: 0.6831 - val_precision_m: 0.6766 - val_recall_m: 0.6921 - val_nist: 0.6504\n",
      "\n",
      "Epoch 00033: val_f1_m did not improve from 0.70906\n",
      "Epoch 34/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0577 - acc: 0.9780 - f1_m: 0.6911 - precision_m: 0.6714 - recall_m: 0.7201 - nist: 0.6455 - val_loss: 0.0760 - val_acc: 0.9760 - val_f1_m: 0.7064 - val_precision_m: 0.6800 - val_recall_m: 0.7375 - val_nist: 0.6206\n",
      "\n",
      "Epoch 00034: val_f1_m did not improve from 0.70906\n",
      "Epoch 35/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0581 - acc: 0.9776 - f1_m: 0.6842 - precision_m: 0.6648 - recall_m: 0.7124 - nist: 0.6613 - val_loss: 0.0746 - val_acc: 0.9753 - val_f1_m: 0.6881 - val_precision_m: 0.6828 - val_recall_m: 0.6957 - val_nist: 0.6365\n",
      "\n",
      "Epoch 00035: val_f1_m did not improve from 0.70906\n",
      "Epoch 36/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0573 - acc: 0.9779 - f1_m: 0.6920 - precision_m: 0.6690 - recall_m: 0.7251 - nist: 0.6492 - val_loss: 0.0744 - val_acc: 0.9763 - val_f1_m: 0.7056 - val_precision_m: 0.6891 - val_recall_m: 0.7258 - val_nist: 0.6126\n",
      "\n",
      "Epoch 00036: val_f1_m did not improve from 0.70906\n",
      "Epoch 37/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0576 - acc: 0.9778 - f1_m: 0.6903 - precision_m: 0.6695 - recall_m: 0.7194 - nist: 0.6438 - val_loss: 0.0759 - val_acc: 0.9750 - val_f1_m: 0.6877 - val_precision_m: 0.6702 - val_recall_m: 0.7090 - val_nist: 0.6479\n",
      "\n",
      "Epoch 00037: val_f1_m did not improve from 0.70906\n",
      "Epoch 38/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0565 - acc: 0.9779 - f1_m: 0.6878 - precision_m: 0.6733 - recall_m: 0.7090 - nist: 0.6442 - val_loss: 0.0750 - val_acc: 0.9761 - val_f1_m: 0.7000 - val_precision_m: 0.6880 - val_recall_m: 0.7140 - val_nist: 0.6168\n",
      "\n",
      "Epoch 00038: val_f1_m did not improve from 0.70906\n",
      "Epoch 39/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0563 - acc: 0.9781 - f1_m: 0.6939 - precision_m: 0.6748 - recall_m: 0.7223 - nist: 0.6387 - val_loss: 0.0771 - val_acc: 0.9747 - val_f1_m: 0.6712 - val_precision_m: 0.6819 - val_recall_m: 0.6678 - val_nist: 0.6583\n",
      "\n",
      "Epoch 00039: val_f1_m did not improve from 0.70906\n",
      "Epoch 40/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0553 - acc: 0.9781 - f1_m: 0.6960 - precision_m: 0.6744 - recall_m: 0.7255 - nist: 0.6333 - val_loss: 0.0760 - val_acc: 0.9763 - val_f1_m: 0.7079 - val_precision_m: 0.6852 - val_recall_m: 0.7343 - val_nist: 0.6123\n",
      "\n",
      "Epoch 00040: val_f1_m did not improve from 0.70906\n",
      "Epoch 41/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0554 - acc: 0.9784 - f1_m: 0.6988 - precision_m: 0.6801 - recall_m: 0.7257 - nist: 0.6266 - val_loss: 0.0782 - val_acc: 0.9760 - val_f1_m: 0.7070 - val_precision_m: 0.6820 - val_recall_m: 0.7373 - val_nist: 0.6193\n",
      "\n",
      "Epoch 00041: val_f1_m did not improve from 0.70906\n",
      "Epoch 42/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0551 - acc: 0.9781 - f1_m: 0.6937 - precision_m: 0.6764 - recall_m: 0.7193 - nist: 0.6363 - val_loss: 0.0784 - val_acc: 0.9756 - val_f1_m: 0.7035 - val_precision_m: 0.6719 - val_recall_m: 0.7409 - val_nist: 0.6312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00042: val_f1_m did not improve from 0.70906\n",
      "Epoch 43/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0544 - acc: 0.9786 - f1_m: 0.7004 - precision_m: 0.6835 - recall_m: 0.7251 - nist: 0.6215 - val_loss: 0.0774 - val_acc: 0.9760 - val_f1_m: 0.7037 - val_precision_m: 0.6852 - val_recall_m: 0.7261 - val_nist: 0.6172\n",
      "\n",
      "Epoch 00043: val_f1_m did not improve from 0.70906\n",
      "Epoch 44/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0543 - acc: 0.9788 - f1_m: 0.7038 - precision_m: 0.6881 - recall_m: 0.7274 - nist: 0.6120 - val_loss: 0.0787 - val_acc: 0.9750 - val_f1_m: 0.6829 - val_precision_m: 0.6754 - val_recall_m: 0.6933 - val_nist: 0.6485\n",
      "\n",
      "Epoch 00044: val_f1_m did not improve from 0.70906\n",
      "Epoch 45/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0538 - acc: 0.9787 - f1_m: 0.6996 - precision_m: 0.6823 - recall_m: 0.7288 - nist: 0.6323 - val_loss: 0.0758 - val_acc: 0.9742 - val_f1_m: 0.6816 - val_precision_m: 0.6541 - val_recall_m: 0.7140 - val_nist: 0.6727\n",
      "\n",
      "Epoch 00045: val_f1_m did not improve from 0.70906\n",
      "Epoch 46/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0539 - acc: 0.9788 - f1_m: 0.7038 - precision_m: 0.6848 - recall_m: 0.7303 - nist: 0.6156 - val_loss: 0.0776 - val_acc: 0.9756 - val_f1_m: 0.6987 - val_precision_m: 0.6759 - val_recall_m: 0.7260 - val_nist: 0.6310\n",
      "\n",
      "Epoch 00046: val_f1_m did not improve from 0.70906\n",
      "Epoch 47/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0539 - acc: 0.9789 - f1_m: 0.7065 - precision_m: 0.6828 - recall_m: 0.7401 - nist: 0.6166 - val_loss: 0.0795 - val_acc: 0.9760 - val_f1_m: 0.6977 - val_precision_m: 0.6873 - val_recall_m: 0.7108 - val_nist: 0.6189\n",
      "\n",
      "Epoch 00047: val_f1_m did not improve from 0.70906\n",
      "Epoch 48/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0526 - acc: 0.9792 - f1_m: 0.7052 - precision_m: 0.6856 - recall_m: 0.7314 - nist: 0.6112 - val_loss: 0.0784 - val_acc: 0.9756 - val_f1_m: 0.7027 - val_precision_m: 0.6733 - val_recall_m: 0.7377 - val_nist: 0.6323\n",
      "\n",
      "Epoch 00048: val_f1_m did not improve from 0.70906\n",
      "Epoch 49/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0524 - acc: 0.9792 - f1_m: 0.7059 - precision_m: 0.6919 - recall_m: 0.7278 - nist: 0.6050 - val_loss: 0.0783 - val_acc: 0.9745 - val_f1_m: 0.6817 - val_precision_m: 0.6696 - val_recall_m: 0.6975 - val_nist: 0.6555\n",
      "\n",
      "Epoch 00049: val_f1_m did not improve from 0.70906\n",
      "Epoch 50/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0521 - acc: 0.9793 - f1_m: 0.7111 - precision_m: 0.6915 - recall_m: 0.7386 - nist: 0.6032 - val_loss: 0.0807 - val_acc: 0.9756 - val_f1_m: 0.7003 - val_precision_m: 0.6729 - val_recall_m: 0.7329 - val_nist: 0.6336\n",
      "\n",
      "Epoch 00050: val_f1_m did not improve from 0.70906\n",
      "Epoch 51/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0517 - acc: 0.9793 - f1_m: 0.7085 - precision_m: 0.6913 - recall_m: 0.7339 - nist: 0.6012 - val_loss: 0.0780 - val_acc: 0.9741 - val_f1_m: 0.6835 - val_precision_m: 0.6603 - val_recall_m: 0.7124 - val_nist: 0.6678\n",
      "\n",
      "Epoch 00051: val_f1_m did not improve from 0.70906\n",
      "Epoch 52/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0517 - acc: 0.9795 - f1_m: 0.7172 - precision_m: 0.6943 - recall_m: 0.7496 - nist: 0.5928 - val_loss: 0.0840 - val_acc: 0.9748 - val_f1_m: 0.6816 - val_precision_m: 0.6758 - val_recall_m: 0.6904 - val_nist: 0.6490\n",
      "\n",
      "Epoch 00052: val_f1_m did not improve from 0.70906\n",
      "Epoch 53/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0513 - acc: 0.9792 - f1_m: 0.7120 - precision_m: 0.6890 - recall_m: 0.7445 - nist: 0.6004 - val_loss: 0.0818 - val_acc: 0.9722 - val_f1_m: 0.6515 - val_precision_m: 0.6376 - val_recall_m: 0.6692 - val_nist: 0.7211\n",
      "\n",
      "Epoch 00053: val_f1_m did not improve from 0.70906\n",
      "Epoch 54/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0512 - acc: 0.9796 - f1_m: 0.7158 - precision_m: 0.6921 - recall_m: 0.7478 - nist: 0.5939 - val_loss: 0.0855 - val_acc: 0.9739 - val_f1_m: 0.6598 - val_precision_m: 0.6703 - val_recall_m: 0.6543 - val_nist: 0.6748\n",
      "\n",
      "Epoch 00054: val_f1_m did not improve from 0.70906\n",
      "Epoch 55/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0510 - acc: 0.9797 - f1_m: 0.7109 - precision_m: 0.6969 - recall_m: 0.7332 - nist: 0.5965 - val_loss: 0.0809 - val_acc: 0.9735 - val_f1_m: 0.6740 - val_precision_m: 0.6513 - val_recall_m: 0.7009 - val_nist: 0.6857\n",
      "\n",
      "Epoch 00055: val_f1_m did not improve from 0.70906\n",
      "Epoch 56/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0507 - acc: 0.9798 - f1_m: 0.7167 - precision_m: 0.7015 - recall_m: 0.7413 - nist: 0.5878 - val_loss: 0.0815 - val_acc: 0.9741 - val_f1_m: 0.6865 - val_precision_m: 0.6563 - val_recall_m: 0.7230 - val_nist: 0.6690\n",
      "\n",
      "Epoch 00056: val_f1_m did not improve from 0.70906\n",
      "Epoch 57/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0503 - acc: 0.9797 - f1_m: 0.7161 - precision_m: 0.6950 - recall_m: 0.7441 - nist: 0.5939 - val_loss: 0.0798 - val_acc: 0.9739 - val_f1_m: 0.6824 - val_precision_m: 0.6539 - val_recall_m: 0.7177 - val_nist: 0.6760\n",
      "\n",
      "Epoch 00057: val_f1_m did not improve from 0.70906\n",
      "Epoch 58/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0499 - acc: 0.9800 - f1_m: 0.7207 - precision_m: 0.7002 - recall_m: 0.7493 - nist: 0.5815 - val_loss: 0.0808 - val_acc: 0.9741 - val_f1_m: 0.6760 - val_precision_m: 0.6606 - val_recall_m: 0.6961 - val_nist: 0.6724\n",
      "\n",
      "Epoch 00058: val_f1_m did not improve from 0.70906\n",
      "Epoch 59/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0502 - acc: 0.9794 - f1_m: 0.7142 - precision_m: 0.6929 - recall_m: 0.7452 - nist: 0.5979 - val_loss: 0.0825 - val_acc: 0.9729 - val_f1_m: 0.6675 - val_precision_m: 0.6493 - val_recall_m: 0.6920 - val_nist: 0.6972\n",
      "\n",
      "Epoch 00059: val_f1_m did not improve from 0.70906\n",
      "Epoch 60/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0493 - acc: 0.9797 - f1_m: 0.7197 - precision_m: 0.6961 - recall_m: 0.7540 - nist: 0.5895 - val_loss: 0.0849 - val_acc: 0.9732 - val_f1_m: 0.6675 - val_precision_m: 0.6530 - val_recall_m: 0.6868 - val_nist: 0.6901\n",
      "\n",
      "Epoch 00060: val_f1_m did not improve from 0.70906\n",
      "Epoch 61/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0493 - acc: 0.9801 - f1_m: 0.7197 - precision_m: 0.7023 - recall_m: 0.7466 - nist: 0.5818 - val_loss: 0.0818 - val_acc: 0.9736 - val_f1_m: 0.6679 - val_precision_m: 0.6614 - val_recall_m: 0.6784 - val_nist: 0.6787\n",
      "\n",
      "Epoch 00061: val_f1_m did not improve from 0.70906\n",
      "Epoch 62/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0489 - acc: 0.9801 - f1_m: 0.7219 - precision_m: 0.6982 - recall_m: 0.7530 - nist: 0.5788 - val_loss: 0.0866 - val_acc: 0.9723 - val_f1_m: 0.6452 - val_precision_m: 0.6447 - val_recall_m: 0.6480 - val_nist: 0.7147\n",
      "\n",
      "Epoch 00062: val_f1_m did not improve from 0.70906\n",
      "Epoch 63/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0490 - acc: 0.9802 - f1_m: 0.7208 - precision_m: 0.7019 - recall_m: 0.7492 - nist: 0.5799 - val_loss: 0.0834 - val_acc: 0.9748 - val_f1_m: 0.6829 - val_precision_m: 0.6730 - val_recall_m: 0.6973 - val_nist: 0.6510\n",
      "\n",
      "Epoch 00063: val_f1_m did not improve from 0.70906\n",
      "Epoch 64/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0481 - acc: 0.9806 - f1_m: 0.7273 - precision_m: 0.7043 - recall_m: 0.7598 - nist: 0.5729 - val_loss: 0.0846 - val_acc: 0.9717 - val_f1_m: 0.6439 - val_precision_m: 0.6420 - val_recall_m: 0.6507 - val_nist: 0.7260\n",
      "\n",
      "Epoch 00064: val_f1_m did not improve from 0.70906\n",
      "Epoch 65/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0477 - acc: 0.9803 - f1_m: 0.7279 - precision_m: 0.7013 - recall_m: 0.7626 - nist: 0.5709 - val_loss: 0.0844 - val_acc: 0.9742 - val_f1_m: 0.6898 - val_precision_m: 0.6567 - val_recall_m: 0.7299 - val_nist: 0.6639\n",
      "\n",
      "Epoch 00065: val_f1_m did not improve from 0.70906\n",
      "Epoch 66/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0493 - acc: 0.9803 - f1_m: 0.7220 - precision_m: 0.7012 - recall_m: 0.7528 - nist: 0.5792 - val_loss: 0.0841 - val_acc: 0.9719 - val_f1_m: 0.6571 - val_precision_m: 0.6306 - val_recall_m: 0.6905 - val_nist: 0.7259\n",
      "\n",
      "Epoch 00066: val_f1_m did not improve from 0.70906\n",
      "Epoch 67/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0479 - acc: 0.9804 - f1_m: 0.7296 - precision_m: 0.6995 - recall_m: 0.7712 - nist: 0.5734 - val_loss: 0.0840 - val_acc: 0.9726 - val_f1_m: 0.6644 - val_precision_m: 0.6420 - val_recall_m: 0.6948 - val_nist: 0.7071\n",
      "\n",
      "Epoch 00067: val_f1_m did not improve from 0.70906\n",
      "Epoch 68/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0475 - acc: 0.9806 - f1_m: 0.7313 - precision_m: 0.7034 - recall_m: 0.7675 - nist: 0.5650 - val_loss: 0.0836 - val_acc: 0.9732 - val_f1_m: 0.6798 - val_precision_m: 0.6445 - val_recall_m: 0.7247 - val_nist: 0.6928\n",
      "\n",
      "Epoch 00068: val_f1_m did not improve from 0.70906\n",
      "Epoch 69/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0472 - acc: 0.9806 - f1_m: 0.7346 - precision_m: 0.7044 - recall_m: 0.7745 - nist: 0.5606 - val_loss: 0.0840 - val_acc: 0.9729 - val_f1_m: 0.6578 - val_precision_m: 0.6576 - val_recall_m: 0.6643 - val_nist: 0.6946\n",
      "\n",
      "Epoch 00069: val_f1_m did not improve from 0.70906\n",
      "Epoch 70/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0472 - acc: 0.9806 - f1_m: 0.7310 - precision_m: 0.7040 - recall_m: 0.7660 - nist: 0.5649 - val_loss: 0.0895 - val_acc: 0.9745 - val_f1_m: 0.6865 - val_precision_m: 0.6690 - val_recall_m: 0.7081 - val_nist: 0.6504\n",
      "\n",
      "Epoch 00070: val_f1_m did not improve from 0.70906\n",
      "Epoch 71/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0463 - acc: 0.9811 - f1_m: 0.7351 - precision_m: 0.7137 - recall_m: 0.7644 - nist: 0.5506 - val_loss: 0.0864 - val_acc: 0.9723 - val_f1_m: 0.6565 - val_precision_m: 0.6397 - val_recall_m: 0.6787 - val_nist: 0.7138\n",
      "\n",
      "Epoch 00071: val_f1_m did not improve from 0.70906\n",
      "Epoch 72/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0465 - acc: 0.9813 - f1_m: 0.7397 - precision_m: 0.7134 - recall_m: 0.7736 - nist: 0.5429 - val_loss: 0.0893 - val_acc: 0.9731 - val_f1_m: 0.6744 - val_precision_m: 0.6452 - val_recall_m: 0.7110 - val_nist: 0.6909\n",
      "\n",
      "Epoch 00072: val_f1_m did not improve from 0.70906\n",
      "Epoch 73/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0459 - acc: 0.9813 - f1_m: 0.7404 - precision_m: 0.7146 - recall_m: 0.7765 - nist: 0.5476 - val_loss: 0.0894 - val_acc: 0.9732 - val_f1_m: 0.6708 - val_precision_m: 0.6526 - val_recall_m: 0.6952 - val_nist: 0.6829\n",
      "\n",
      "Epoch 00073: val_f1_m did not improve from 0.70906\n",
      "Epoch 74/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0461 - acc: 0.9815 - f1_m: 0.7430 - precision_m: 0.7135 - recall_m: 0.7818 - nist: 0.5416 - val_loss: 0.0903 - val_acc: 0.9711 - val_f1_m: 0.6092 - val_precision_m: 0.6461 - val_recall_m: 0.5804 - val_nist: 0.7477\n",
      "\n",
      "Epoch 00074: val_f1_m did not improve from 0.70906\n",
      "Epoch 75/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0460 - acc: 0.9814 - f1_m: 0.7398 - precision_m: 0.7203 - recall_m: 0.7685 - nist: 0.5425 - val_loss: 0.0863 - val_acc: 0.9731 - val_f1_m: 0.6670 - val_precision_m: 0.6482 - val_recall_m: 0.6907 - val_nist: 0.6901\n",
      "\n",
      "Epoch 00075: val_f1_m did not improve from 0.70906\n",
      "Epoch 76/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0464 - acc: 0.9810 - f1_m: 0.7355 - precision_m: 0.7135 - recall_m: 0.7658 - nist: 0.5509 - val_loss: 0.0880 - val_acc: 0.9732 - val_f1_m: 0.6671 - val_precision_m: 0.6548 - val_recall_m: 0.6855 - val_nist: 0.6894\n",
      "\n",
      "Epoch 00076: val_f1_m did not improve from 0.70906\n",
      "Epoch 77/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0455 - acc: 0.9813 - f1_m: 0.7414 - precision_m: 0.7175 - recall_m: 0.7755 - nist: 0.5428 - val_loss: 0.0905 - val_acc: 0.9736 - val_f1_m: 0.6752 - val_precision_m: 0.6549 - val_recall_m: 0.6996 - val_nist: 0.6745\n",
      "\n",
      "Epoch 00077: val_f1_m did not improve from 0.70906\n",
      "Epoch 78/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0450 - acc: 0.9813 - f1_m: 0.7418 - precision_m: 0.7130 - recall_m: 0.7794 - nist: 0.5418 - val_loss: 0.0903 - val_acc: 0.9723 - val_f1_m: 0.6578 - val_precision_m: 0.6446 - val_recall_m: 0.6769 - val_nist: 0.7101\n",
      "\n",
      "Epoch 00078: val_f1_m did not improve from 0.70906\n",
      "Epoch 79/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0450 - acc: 0.9816 - f1_m: 0.7439 - precision_m: 0.7152 - recall_m: 0.7804 - nist: 0.5398 - val_loss: 0.0909 - val_acc: 0.9728 - val_f1_m: 0.6632 - val_precision_m: 0.6451 - val_recall_m: 0.6871 - val_nist: 0.7012\n",
      "\n",
      "Epoch 00079: val_f1_m did not improve from 0.70906\n",
      "Epoch 80/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0441 - acc: 0.9818 - f1_m: 0.7492 - precision_m: 0.7227 - recall_m: 0.7864 - nist: 0.5269 - val_loss: 0.0901 - val_acc: 0.9732 - val_f1_m: 0.6708 - val_precision_m: 0.6497 - val_recall_m: 0.6987 - val_nist: 0.6901\n",
      "\n",
      "Epoch 00080: val_f1_m did not improve from 0.70906\n",
      "Epoch 81/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0442 - acc: 0.9819 - f1_m: 0.7488 - precision_m: 0.7195 - recall_m: 0.7874 - nist: 0.5250 - val_loss: 0.0939 - val_acc: 0.9732 - val_f1_m: 0.6690 - val_precision_m: 0.6494 - val_recall_m: 0.6937 - val_nist: 0.6915\n",
      "\n",
      "Epoch 00081: val_f1_m did not improve from 0.70906\n",
      "Epoch 82/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0435 - acc: 0.9823 - f1_m: 0.7527 - precision_m: 0.7255 - recall_m: 0.7885 - nist: 0.5198 - val_loss: 0.0922 - val_acc: 0.9736 - val_f1_m: 0.6700 - val_precision_m: 0.6585 - val_recall_m: 0.6865 - val_nist: 0.6776\n",
      "\n",
      "Epoch 00082: val_f1_m did not improve from 0.70906\n",
      "Epoch 83/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0439 - acc: 0.9820 - f1_m: 0.7520 - precision_m: 0.7190 - recall_m: 0.7950 - nist: 0.5275 - val_loss: 0.0934 - val_acc: 0.9738 - val_f1_m: 0.6711 - val_precision_m: 0.6660 - val_recall_m: 0.6822 - val_nist: 0.6736\n",
      "\n",
      "Epoch 00083: val_f1_m did not improve from 0.70906\n",
      "Epoch 84/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0439 - acc: 0.9816 - f1_m: 0.7443 - precision_m: 0.7275 - recall_m: 0.7703 - nist: 0.5300 - val_loss: 0.0945 - val_acc: 0.9732 - val_f1_m: 0.6792 - val_precision_m: 0.6460 - val_recall_m: 0.7203 - val_nist: 0.6876\n",
      "\n",
      "Epoch 00084: val_f1_m did not improve from 0.70906\n",
      "Epoch 85/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0431 - acc: 0.9820 - f1_m: 0.7477 - precision_m: 0.7190 - recall_m: 0.7865 - nist: 0.5350 - val_loss: 0.0943 - val_acc: 0.9710 - val_f1_m: 0.6395 - val_precision_m: 0.6314 - val_recall_m: 0.6544 - val_nist: 0.7455\n",
      "\n",
      "Epoch 00085: val_f1_m did not improve from 0.70906\n",
      "Epoch 86/1000\n",
      "61237/61237 [==============================] - 1s 9us/step - loss: 0.0438 - acc: 0.9819 - f1_m: 0.7468 - precision_m: 0.7244 - recall_m: 0.7779 - nist: 0.5299 - val_loss: 0.0935 - val_acc: 0.9729 - val_f1_m: 0.6718 - val_precision_m: 0.6419 - val_recall_m: 0.7075 - val_nist: 0.6991\n",
      "\n",
      "Epoch 00086: val_f1_m did not improve from 0.70906\n",
      "Epoch 87/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0427 - acc: 0.9823 - f1_m: 0.7580 - precision_m: 0.7277 - recall_m: 0.7961 - nist: 0.5095 - val_loss: 0.0938 - val_acc: 0.9713 - val_f1_m: 0.6422 - val_precision_m: 0.6275 - val_recall_m: 0.6602 - val_nist: 0.7406\n",
      "\n",
      "Epoch 00087: val_f1_m did not improve from 0.70906\n",
      "Epoch 88/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0426 - acc: 0.9825 - f1_m: 0.7597 - precision_m: 0.7287 - recall_m: 0.8020 - nist: 0.5084 - val_loss: 0.0964 - val_acc: 0.9716 - val_f1_m: 0.6355 - val_precision_m: 0.6399 - val_recall_m: 0.6352 - val_nist: 0.7301\n",
      "\n",
      "Epoch 00088: val_f1_m did not improve from 0.70906\n",
      "Epoch 89/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0427 - acc: 0.9823 - f1_m: 0.7533 - precision_m: 0.7259 - recall_m: 0.7892 - nist: 0.5199 - val_loss: 0.0989 - val_acc: 0.9722 - val_f1_m: 0.6421 - val_precision_m: 0.6493 - val_recall_m: 0.6374 - val_nist: 0.7150\n",
      "\n",
      "Epoch 00089: val_f1_m did not improve from 0.70906\n",
      "Epoch 90/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0426 - acc: 0.9822 - f1_m: 0.7516 - precision_m: 0.7284 - recall_m: 0.7846 - nist: 0.5209 - val_loss: 0.0992 - val_acc: 0.9716 - val_f1_m: 0.6356 - val_precision_m: 0.6393 - val_recall_m: 0.6358 - val_nist: 0.7313\n",
      "\n",
      "Epoch 00090: val_f1_m did not improve from 0.70906\n",
      "Epoch 91/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0421 - acc: 0.9828 - f1_m: 0.7606 - precision_m: 0.7365 - recall_m: 0.7941 - nist: 0.5020 - val_loss: 0.0982 - val_acc: 0.9728 - val_f1_m: 0.6706 - val_precision_m: 0.6435 - val_recall_m: 0.7028 - val_nist: 0.6974\n",
      "\n",
      "Epoch 00091: val_f1_m did not improve from 0.70906\n",
      "Epoch 92/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0416 - acc: 0.9824 - f1_m: 0.7551 - precision_m: 0.7300 - recall_m: 0.7904 - nist: 0.5201 - val_loss: 0.0965 - val_acc: 0.9725 - val_f1_m: 0.6477 - val_precision_m: 0.6484 - val_recall_m: 0.6501 - val_nist: 0.7080\n",
      "\n",
      "Epoch 00092: val_f1_m did not improve from 0.70906\n",
      "Epoch 93/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0405 - acc: 0.9831 - f1_m: 0.7656 - precision_m: 0.7412 - recall_m: 0.7988 - nist: 0.4902 - val_loss: 0.0985 - val_acc: 0.9725 - val_f1_m: 0.6550 - val_precision_m: 0.6438 - val_recall_m: 0.6701 - val_nist: 0.7101\n",
      "\n",
      "Epoch 00093: val_f1_m did not improve from 0.70906\n",
      "Epoch 94/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0416 - acc: 0.9824 - f1_m: 0.7552 - precision_m: 0.7298 - recall_m: 0.7882 - nist: 0.5120 - val_loss: 0.0986 - val_acc: 0.9708 - val_f1_m: 0.6522 - val_precision_m: 0.6146 - val_recall_m: 0.7001 - val_nist: 0.7533\n",
      "\n",
      "Epoch 00094: val_f1_m did not improve from 0.70906\n",
      "Epoch 95/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0431 - acc: 0.9832 - f1_m: 0.7624 - precision_m: 0.7449 - recall_m: 0.7924 - nist: 0.4941 - val_loss: 0.0987 - val_acc: 0.9711 - val_f1_m: 0.6434 - val_precision_m: 0.6281 - val_recall_m: 0.6634 - val_nist: 0.7398\n",
      "\n",
      "Epoch 00095: val_f1_m did not improve from 0.70906\n",
      "Epoch 96/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0411 - acc: 0.9830 - f1_m: 0.7664 - precision_m: 0.7387 - recall_m: 0.8026 - nist: 0.4879 - val_loss: 0.0992 - val_acc: 0.9722 - val_f1_m: 0.6579 - val_precision_m: 0.6370 - val_recall_m: 0.6831 - val_nist: 0.7175\n",
      "\n",
      "Epoch 00096: val_f1_m did not improve from 0.70906\n",
      "Epoch 97/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0406 - acc: 0.9828 - f1_m: 0.7614 - precision_m: 0.7345 - recall_m: 0.7983 - nist: 0.5026 - val_loss: 0.0986 - val_acc: 0.9716 - val_f1_m: 0.6340 - val_precision_m: 0.6453 - val_recall_m: 0.6285 - val_nist: 0.7271\n",
      "\n",
      "Epoch 00097: val_f1_m did not improve from 0.70906\n",
      "Epoch 98/1000\n",
      "61237/61237 [==============================] - 1s 10us/step - loss: 0.0404 - acc: 0.9829 - f1_m: 0.7645 - precision_m: 0.7314 - recall_m: 0.8074 - nist: 0.5003 - val_loss: 0.0986 - val_acc: 0.9713 - val_f1_m: 0.6371 - val_precision_m: 0.6355 - val_recall_m: 0.6423 - val_nist: 0.7356\n",
      "\n",
      "Epoch 00098: val_f1_m did not improve from 0.70906\n",
      "Epoch 99/1000\n",
      "19000/61237 [========>.....................] - ETA: 0s - loss: 0.0405 - acc: 0.9836 - f1_m: 0.7728 - precision_m: 0.7570 - recall_m: 0.7951 - nist: 0.4648"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-1e6f80dba2b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#             early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6, min_delta=0.001)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_ts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/local/scratch/mac224/anaconda/envs/SUNDER/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/local/scratch/mac224/anaconda/envs/SUNDER/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/mac224/anaconda/envs/SUNDER/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/mac224/anaconda/envs/SUNDER/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_SESSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/mac224/anaconda/envs/SUNDER/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/scratch/mac224/anaconda/envs/SUNDER/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5649\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5650\u001b[0m       with super(_DefaultGraphStack,\n\u001b[0;32m-> 5651\u001b[0;31m                  self).get_controller(default) as g, context.graph_mode():\n\u001b[0m\u001b[1;32m   5652\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5653\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weights = class_weight.compute_class_weight('balanced',\n",
    "                                            np.unique(y_tr),\n",
    "                                            np.array(y_tr).reshape(-1))\n",
    "weigths = {index: value for index, value in enumerate(weights)}\n",
    "model = FC_model()\n",
    "\n",
    "name = f'BULATS-old-features-deeper-bigger-network-automatic-weights-{datetime.now().strftime(\"%d_%m_%Y-%H_%M_%S\")}'\n",
    "checkpoint = ModelCheckpoint(f'training/{name}.h5', monitor='val_f1_m', verbose=1, save_best_only=True, mode='max')\n",
    "tensorboard = TensorBoard(log_dir=f\"training/tensorboard/{name}\")\n",
    "#             early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=6, min_delta=0.001)\n",
    "\n",
    "model.fit(x_tr, y_tr, epochs=1000, batch_size=1000, callbacks=[checkpoint, tensorboard], validation_data=(x_ts, y_ts), shuffle=True, class_weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once trained (and tested using previously defined functions) we can evaluate which features seem to be the most important ones. For that we use Lasso method where here we evaluate the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha using built-in LassoCV: 0.002845\n",
      "Best score using built-in LassoCV: 0.300670\n"
     ]
    }
   ],
   "source": [
    "reg = LassoCV(cv=5)\n",
    "reg.fit(train.loc[:, 'prePause':'EminWplus1'], train['boundary'])\n",
    "print(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\n",
    "print(\"Best score using built-in LassoCV: %f\" %reg.score(train.loc[:, 'prePause':'EminWplus1'], train['boundary']))\n",
    "coef = pd.Series(reg.coef_, index = train.loc[:, 'prePause':'EminWplus1'].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of features rejected shows how clean the data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso picked 17 variables and eliminated the other 6 variables\n"
     ]
    }
   ],
   "source": [
    "print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To know the exact influence of all the features we plot them in a nice graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Feature importance using Lasso Model')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imp_coef = coef.sort_values()\n",
    "matplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "imp_coef.plot(kind = \"barh\")\n",
    "plt.title(\"Feature importance using Lasso Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to generate the fsts for the further processing we need to use the externally created functions in the models.py file. Those will generate the necessary fst's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import create_slm, create_fst_prosodic, create_fst_slm\n",
    "\n",
    "lengths = []\n",
    "current = 0\n",
    "for i in train['boundary']:\n",
    "    if i == 0:\n",
    "        current += 1\n",
    "    elif i == 1:\n",
    "        lengths.append(current)\n",
    "        current = 0\n",
    "\n",
    "model = load_model(\"training/BULATS-old-features-bigger-network-automatic-weights-04_09_2019-02_30_55.h5\", custom_objects={'f1_m': f1_m, 'precision_m': precision_m, 'recall_m': recall_m, 'nist': nist})\n",
    "slm, max_seen = create_slm(lengths=lengths)\n",
    "slm_non_zero, slm_zero = create_fst_slm(lengths, test['token'])\n",
    "for recording in test['recording'].unique():\n",
    "    x_ts = scaler.transform(test.loc[test['recording'] == recording, 'prePause':'EminWplus1'])\n",
    "    prosodic_non_zero, prosodic_zero = create_fst_prosodic(model, x_ts, test.loc[test['recording'] == recording, 'token'])\n",
    "    fst.compose(prosodic_zero, slm.arcsort())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
